\chapter{Conclusion} % Main chapter title

\label{chap:conclusion} % For referencing the chapter elsewhere, use \ref{chap:background}

In this thesis, we have introduced three methods that apply \acp{dnn} as an alternative to \acp{gp} for the surrogate models in \ac{bo}. We also extended the use of \ac{dnn} to different settings of \ac{bo}. These methods improved the scalability drawback of \ac{bo} with \ac{gp} while ensuring the optimization convergence. Further, \acp{dnn}-based \acl{bo} shows the potential application to high-dimensional structural data. We summarize our
contributions below and outline some potential future work in this area. 

\section{Contributions}
To address the computational costs and scalability limitations (particularly for structural inputs such as images or text, which are often accompanied by high-dimensional data), caused by the cubic complexity of kernel inversion in \acp{gp} used for \ac{bo}, this thesis focused on developing methods based on \acp{dnn} for both standard and extended \ac{bo} problems.

The key idea was first applied within the standard \ac{bo} setting. In \textbf{Chapter} \ref{chap:neural-bo}, a novel black-box optimization algorithm was introduced, where the unknown function was modeled using an over-parameterized \ac{dnn}. \ac{ntk} theory was employed to control network uncertainty, along with the \ac{ts} technique for selecting the next point for evaluation. Theoretical analysis of the algorithm's regret bound was demonstrated to show convergence and improved sample efficiency over existing methods. Additionally, the Neural-BO algorithm was shown to be scalable, with computational costs growing only linearly with the number of data points, making it suitable for optimizing objective functions with complex structured inputs like images and texts. Experimental results on synthetic benchmarks and real-world optimization tasks confirmed that the algorithm outperformed current state-of-the-art methods in \ac{bo}. 

In \textbf{Chapter} \ref{chap:neural-cbo}, the \ac{dnn}-based approach was extended to address \ac{bo} with unknown, expensive constraints. Neural-CBO was introduced, where both the objective function and constraints were modeled by \acp{dnn}. \ac{ei} was used as the acquisition function for the objective, while the feasible region was defined using Lower Confidence Bound (LCB) conditions. Theoretical analysis indicated that cumulative regret and constraint violations had upper bounds comparable to those of \ac{gp}-based methods, under a more relaxed condition on network width. Experimental verification, conducted on synthetic and real-world tasks, demonstrated that Neural-CBO performed competitively with recent state-of-the-art approaches. 

In \textbf{Chapter} \ref{chap:pinn-bo}, a novel problem setting for \ac{bo} was introduced, integrating physical prior knowledge about the underlying black-box function through \acp{pde}, using \acp{pinn}. The PINN-BO algorithm enhanced sample efficiency by efficiently handling a broad class of \acp{pde}, thereby promoting the use of physical knowledge in \ac{bo}. It was ultimately demonstrated that this approach significantly improved practical performance when compared to existing methods (without incorporating PDE priors) across various synthetic and real-world benchmark functions.
 
\section{Future Directions}
In this thesis, the convergence of \ac{dnn}-based \ac{bo} approaches is demonstrated in terms of the maximum information gain, denoted as $\gamma_T$, with respect to the \ac{ntk}. As established by \citet{kassraie2022neural}, $\gamma_T$ is bounded by $\mathcal{O}(T^{1-d^{-1}})$, where $T$ represents the number of observations and $d$ the input dimensionality. In \textbf{Chapter} \ref{chap:neural-bo}, this result was utilized to prove that the Neural-BO algorithm achieves a sub-linear regret bound, which is crucial for ensuring efficient optimization. This result is based on the assumption that the observation noise is independent of the observed values. However, in more complex scenarios where the noise depends on prior observations, deriving a sublinear regret bound remains an open problem, suggesting an interesting avenue for future research, either by improving the upper bound of $\gamma_T$ or by designing a more refined algorithm to ensure sublinear convergence.

A related and equally promising direction for future work is the improvement of the theoretical analysis of PINN-BO, as discussed in \textbf{Chapter} \ref{chap:pinn-bo}. The current regret bound for this algorithm includes the interaction information value, which is influenced by the \ac{pde} observations, making the regret bound dependent on the specific \ac{pde}. While it has been shown that PINN-BO is capable of handling a broad class of \ac{pde} constraints, establishing a regret bound for each specific \ac{pde} class, such as linear \acp{pde}, remains a significant challenge. In Section \ref{section:pinn-bo_theoretical_analysis}, a regret bound is provided for a simple class of \acp{pde}, yet extending this analysis to more complex classes could significantly improve the understanding of how physical information, expressed through \acp{pde}, positively influences the optimization process. By analyzing the regret bound for each \ac{pde} class, the integration of physical knowledge into \acl{bo} can be deepened, thereby enhancing the efficiency and practicality of algorithms like PINN-BO. This approach would not only strengthen the theoretical foundations of PINN-BO but also broaden its application to real-world optimization problems governed by diverse physical laws.      
