\chapter*{Abstract} % Main chapter title
\addcontentsline{toc}{chapter}{Abstract}
\label{chap:abstract}
Optimization has become an essential task across numerous fields, as many systems and processes require fine-tuning to achieve maximum efficiency. In many cases, these systems are ``black-box'', meaning that the underlying mechanisms are unknown or too complex to model explicitly; we can only provide inputs and observe the resulting outputs. The challenge with optimizing black-box systems is that evaluating them can often be costly, both in terms of time and resources. This creates a demand for optimization methods that are not only effective but also sample-efficient, minimizing the number of evaluations needed to find optimal solutions. 


Bayesian optimization or \ac{bo} has emerged as a leading framework for optimizing expensive and unknown functions in various fields, including materials science, biomedical research, and machine learning. BO typically relies on two core components: (1) a surrogate model that approximates the underlying function based on available observations, and (2) an acquisition function that balances the trade-off between exploration (searching unexplored regions with high uncertainty) and exploitation (refining known promising regions). Traditionally, \ac{gp} has been the preferred choice for the surrogate model due to its ability to quantify uncertainty and its analytic expression, hence being convenient to provide theoretical guarantees. However, \acp{gp} suffer from cubic computational complexity in relation to the number of observations, making them impractical for large-scale applications.


First of all, investigating alternative surrogate models to enhance the performance of standard \ac{bo}, especially in scenarios where \ac{gp} are computationally prohibitive, is focused. 
The ability of \acp{dnn} to scale linearly with the number of data points and capture complex patterns makes them more suitable for tasks involving structural data (like images and text). We introduce \textbf{Neural-BO}, a \ac{dnn}-based optimization algorithm that leverages recent advances in neural network theory to estimate uncertainty and uses \ac{ts} for next point selection. This method maintains the flexibility of \acp{dnn} while achieving convergence guarantees through the \ac{ntk}-based theory, showing improved sample efficiency and faster convergence.   

However, optimizing real-world functions often goes beyond simple maximization or minimization, requiring the incorporation of constraints. These constraints ensure that solutions not only optimize the objective but also meet critical feasibility criteria, which is especially important in domains like engineering and physics. Hence, we extend the \ac{dnn}-based approach to handle optimization problems with expensive, unknown constraints through \textbf{Neural-CBO}. In this method, both the objective function and the constraints are modeled using DNNs, and the acquisition function is designed to balance exploration and exploitation while ensuring constraint satisfaction. By using \ac{lcb} conditions to guarantee feasibility and the \ac{ei} acquisition function to guide the search, Neural-CBO efficiently explores the feasible region, even when it is significantly smaller than the overall search space. Our method provides upper bounds on both regret and constraint violations, offering a theoretically sound and scalable approach for constrained black-box optimization tasks.

Moreover, in fields like physics, engineering, and biology, there is often domain-specific knowledge available, typically in the form of physical laws or models, which can be expressed as \acp{pde}. Incorporating physical knowledge into the optimization process enhances function approximation by providing more accurate guidance, improving surrogate model predictions, and reducing uncertainty in regions that follow known physical laws. Therefore, we propose PINN-BO, an approach that integrates \ac{pinn} into the \ac{bo} framework.  By incorporating these physical constraints into the optimization process, PINN-BO significantly improves sample efficiency. The method employs \ac{pinn} to model the objective function, utilizing known physical principles by embedding the \ac{pde} into the network's training process. This ensures that the optimization process not only converges quickly but also remains aligned with the underlying physical system. Experimental results on both synthetic and real-world tasks demonstrate the superior performance of PINN-BO in domains where physical knowledge is crucial. 

In summary, this thesis proposed three novel methods, each leveraging \acp{dnn}-based surrogate models to enhance computation complexity and sample efficiency in \ac{bo} as well as its extended settings.

% \paragraph{\huge List of Publications}
% \begin{enumerate}
%     \item Phan-Trong, D., Tran-The, H., \& Gupta, S. (2023). NeuralBO: A black-box optimization algorithm using deep neural networks. Neurocomputing, 559, 126776.
%     \item Phan-Trong, D., Tran, H. T., Shilton, A., \& Gupta, S. (2024, August). PINN-BO: A Black-Box Optimization Algorithm Using Physics-Informed Neural Networks. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 357-374). Cham: Springer Nature Switzerland.
%     \item Phan-Trong, D., Tran, H. T., \& Gupta, S. (2024). Black-box Optimization with Unknown Black-box Constraints via Overparametrized Deep Neural Networks. Submitted to AISTATS 2025.  
% \end{enumerate}

