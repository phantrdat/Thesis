\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks}{48}{chapter.187}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:neural-bo}{{3}{48}{Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks}{chapter.187}{}}
\acronymused{gp}
\acronymused{bo}
\acronymused{gp}
\acronymused{bo}
\acronymused{gp}
\acronymused{gp}
\acronymused{dnn}
\acronymused{gp}
\acronymused{dnn}
\acronymused{bnn}
\acronymused{dnn}
\acronymused{bnn}
\acronymused{ntk}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Problem Setting}{49}{section.188}\protected@file@percent }
\newlabel{section:neural-bo_problem_setting}{{3.1}{49}{Problem Setting}{section.188}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Proposed Neural-BO Method}{49}{section.189}\protected@file@percent }
\newlabel{section:neural-bo_proposed_method}{{3.2}{49}{Proposed Neural-BO Method}{section.189}{}}
\acronymused{dnn}
\acronymused{bo}
\acronymused{gp}
\AC@undonewlabel{acro:relu}
\newlabel{acro:relu}{{3.2}{49}{Proposed Neural-BO Method}{section.189}{}}
\acronymused{relu}
\@writefile{toc}{\contentsline {paragraph}{Discussion}{50}{section*.190}\protected@file@percent }
\acronymused{dnn}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Neural Black-box Optimization (Neural-BO)}}{50}{algorithm.191}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:Neural-BO}{{1}{50}{Neural Black-box Optimization (Neural-BO)}{algorithm.191}{}}
\newlabel{line:calculate_var}{{3}{50}{Neural Black-box Optimization (Neural-BO)}{algorithm.191}{}}
\newlabel{line:train_NN}{{6}{50}{Neural Black-box Optimization (Neural-BO)}{algorithm.191}{}}
\newlabel{line:update_Ut}{{7}{50}{Neural Black-box Optimization (Neural-BO)}{algorithm.191}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces TrainNN}}{51}{algorithm.192}\protected@file@percent }
\newlabel{alg:train_NN}{{2}{51}{TrainNN}{algorithm.192}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Theoretical Analysis}{51}{section.193}\protected@file@percent }
\newlabel{section:neural-bo_regret_analysis}{{3.3}{51}{Theoretical Analysis}{section.193}{}}
\acronymused{ntk}
\acronymused{gp}
\acronymused{ts}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~3.3.1\else \numberline {3.3.1}Definition\fi \thmtformatoptarg {\blx@tocontentsinit {0}\cite {jacot2018neural}}}{51}{definition.195}\protected@file@percent }
\newlabel{def:NTK_matrix}{{3.3.1}{51}{\cite {jacot2018neural}}{definition.195}{}}
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~3.3.2\else \numberline {3.3.2}Assumption\fi }{51}{assumption.197}\protected@file@percent }
\newlabel{assumption:sufficient_exploration}{{3.3.2}{51}{}{assumption.197}{}}
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~3.3.3\else \numberline {3.3.3}Assumption\fi }{51}{assumption.199}\protected@file@percent }
\acronymused{rkhs}
\acronymused{ntk}
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~3.3.4\else \numberline {3.3.4}Assumption\fi }{51}{assumption.201}\protected@file@percent }
\newlabel{assumption:iid_noise}{{3.3.4}{51}{}{assumption.201}{}}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~3.3.5\else \numberline {3.3.5}Theorem\fi }{52}{theorem.203}\protected@file@percent }
\newlabel{theorem:neural-bo_main}{{3.3.5}{52}{}{theorem.203}{}}
\@writefile{loe}{\contentsline {remark}{\ifthmt@listswap Remark~3.3.6\else \numberline {3.3.6}Remark\fi }{52}{remark.205}\protected@file@percent }
\acronymused{ntk}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Proof of the Main Theorem}{52}{section.206}\protected@file@percent }
\newlabel{eqn:rkhs_lipschitz}{{3.1}{53}{Proof of the Main Theorem}{equation.207}{}}
\newlabel{proof:theorem_main}{{3.4}{53}{Proof of the Main Theorem}{equation.207}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Experiments}{55}{section.208}\protected@file@percent }
\newlabel{section:neural-bo_experiments}{{3.5}{55}{Experiments}{section.208}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Experimental Setup}{55}{subsection.209}\protected@file@percent }
\acronymused{gp}
\acronymused{rf}
\acronymused{dnn}
\acronymused{gp}
\acronymused{rf}
\acronymused{dnn}
\acronymused{rf}
\acronymused{dnn}
\acronymused{rf}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Synthetic Benchmarks}{56}{subsection.214}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The plots show the minimum true value observed after optimizing several synthetic functions over 2000 iterations of our proposed algorithm and 6 baselines. The dimension of each function is shown in the parenthesis.}}{56}{figure.caption.215}\protected@file@percent }
\newlabel{fig:neural-bo_synthetic}{{3.1}{56}{The plots show the minimum true value observed after optimizing several synthetic functions over 2000 iterations of our proposed algorithm and 6 baselines. The dimension of each function is shown in the parenthesis}{figure.caption.215}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces The p-values of KS-test "whether the data obtained from running our methods Neural-BO and all baselines are normally distributed".}}{57}{table.caption.216}\protected@file@percent }
\newlabel{tab:ks_test}{{3.1}{57}{The p-values of KS-test "whether the data obtained from running our methods Neural-BO and all baselines are normally distributed"}{table.caption.216}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces One-sided t-tests were employed to assess whether the baseline achieves a lower function value compared to our proposed method, Neural-BO. The null hypothesis $H_0: \mu _\text  {baseline} \le \mu _{\text  {Neural-BO}}$ and the alternative hypothesis: $H_a: \mu _\text  {baseline} > \mu _{\text  {Neural-BO}}$. The p-value corresponding to each test is provided as the first value in each cell. Moreover, to account for multiple hypotheses testing, the Benjamini-Hochberg correction was applied and is reported as the second value in each cell. In the outcome, a "T" indicates that the null hypothesis is rejected, whereas an "F" signifies that it is not rejected.}}{58}{table.caption.217}\protected@file@percent }
\newlabel{tab:t-test}{{3.2}{58}{One-sided t-tests were employed to assess whether the baseline achieves a lower function value compared to our proposed method, Neural-BO. The null hypothesis $H_0: \mu _\text {baseline} \le \mu _{\text {Neural-BO}}$ and the alternative hypothesis: $H_a: \mu _\text {baseline} > \mu _{\text {Neural-BO}}$. The p-value corresponding to each test is provided as the first value in each cell. Moreover, to account for multiple hypotheses testing, the Benjamini-Hochberg correction was applied and is reported as the second value in each cell. In the outcome, a "T" indicates that the null hypothesis is rejected, whereas an "F" signifies that it is not rejected}{table.caption.217}{}}
\newlabel{fig:coco_2d}{{3.2a}{59}{2D}{figure.caption.219}{}}
\newlabel{sub@fig:coco_2d}{{a}{59}{2D}{figure.caption.219}{}}
\newlabel{fig:coco_3d}{{3.2b}{59}{3D}{figure.caption.219}{}}
\newlabel{sub@fig:coco_3d}{{b}{59}{3D}{figure.caption.219}{}}
\newlabel{fig:coco_5d}{{3.2c}{59}{5D}{figure.caption.219}{}}
\newlabel{sub@fig:coco_5d}{{c}{59}{5D}{figure.caption.219}{}}
\newlabel{fig:coco_10d}{{3.2d}{59}{10D}{figure.caption.219}{}}
\newlabel{sub@fig:coco_10d}{{d}{59}{10D}{figure.caption.219}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The results of benchmarking our Neural-BO and the baselines with COCO framework on 24 BBOB noiseless objective functions with four different dimensions \{2,3,5,10\}.}}{59}{figure.caption.219}\protected@file@percent }
\newlabel{fig:coco}{{3.2}{59}{The results of benchmarking our Neural-BO and the baselines with COCO framework on 24 BBOB noiseless objective functions with four different dimensions \{2,3,5,10\}}{figure.caption.219}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Real-world Applications}{60}{subsection.220}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.1}Designing Sensitive Samples for Detection of Model Tampering}{60}{subsubsection.221}\protected@file@percent }
\newlabel{section:neural-bo_sensitive_samples}{{3.5.3.1}{60}{Designing Sensitive Samples for Detection of Model Tampering}{subsubsection.221}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The plot shows the \textbf  {detection rates} corresponding to the number of samples on the MNIST dataset. The larger the number of sensitive samples, the higher the detection rate. As shown in the figure, Neural-BO can generate sensitive samples that achieve nearly 90\% of the detection rate with at least 8 samples.}}{60}{figure.caption.222}\protected@file@percent }
\newlabel{fig:neural-bo_sensitive_sample}{{3.3}{60}{The plot shows the \textbf {detection rates} corresponding to the number of samples on the MNIST dataset. The larger the number of sensitive samples, the higher the detection rate. As shown in the figure, Neural-BO can generate sensitive samples that achieve nearly 90\% of the detection rate with at least 8 samples}{figure.caption.222}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.2}Unknown target document retrieval}{61}{subsubsection.223}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces We search for the most related document for a specified target document in \textbf  {Amazon product reviews} dataset and report the maximum \textbf  {hierachical F1 score} found by all baselines. All methods show similar behaviour and Neural-BO performs comparably and much better than GP-based baselines.}}{61}{figure.caption.224}\protected@file@percent }
\newlabel{fig:text}{{3.4}{61}{We search for the most related document for a specified target document in \textbf {Amazon product reviews} dataset and report the maximum \textbf {hierachical F1 score} found by all baselines. All methods show similar behaviour and Neural-BO performs comparably and much better than GP-based baselines}{figure.caption.224}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.3}Optimizing control parameters for robot pushing}{62}{subsubsection.226}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces  Optimization results for control parameters of 14D robot pushing problem. The X-axis shows iterations, and the y-axis shows the median of the best reward obtained.}}{62}{figure.caption.227}\protected@file@percent }
\newlabel{fig:robot_14D}{{3.5}{62}{Optimization results for control parameters of 14D robot pushing problem. The X-axis shows iterations, and the y-axis shows the median of the best reward obtained}{figure.caption.227}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion}{63}{section.228}\protected@file@percent }
\acronymused{bo}
\acronymused{dnn}
\acronymused{gp}
\@setckpt{Chapters/neural-bo}{
\setcounter{page}{64}
\setcounter{equation}{1}
\setcounter{enumi}{2}
\setcounter{enumii}{4}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{6}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{2}
\setcounter{LT@tables}{1}
\setcounter{LT@chunks}{1}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{143}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{1}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALG@line}{4}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{parentequation}{0}
\setcounter{thmt@dummyctr}{7}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{theorem}{0}
\setcounter{sublemma}{0}
\setcounter{section@level}{1}
\setcounter{Item}{38}
\setcounter{Hfootnote}{6}
\setcounter{bookmark@seq@number}{81}
}
