\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks}{30}{chapter.85}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:neural-bo}{{3}{30}{Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks}{chapter.85}{}}
\acronymused{gp}
\acronymused{bo}
\acronymused{gp}
\acronymused{bo}
\acronymused{gp}
\acronymused{gp}
\acronymused{dnn}
\acronymused{gp}
\acronymused{dnn}
\acronymused{bnn}
\acronymused{dnn}
\acronymused{bnn}
\acronymused{ntk}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Problem Setting}{31}{section.86}\protected@file@percent }
\newlabel{section:neural-bo_problem_setting}{{3.1}{31}{Problem Setting}{section.86}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Proposed Neural-BO Method}{31}{section.87}\protected@file@percent }
\newlabel{section:neural-bo_proposed_method}{{3.2}{31}{Proposed Neural-BO Method}{section.87}{}}
\acronymused{dnn}
\acronymused{bo}
\acronymused{gp}
\AC@undonewlabel{acro:relu}
\newlabel{acro:relu}{{3.2}{31}{Proposed Neural-BO Method}{section.87}{}}
\acronymused{relu}
\@writefile{toc}{\contentsline {paragraph}{Discussion}{32}{section*.88}\protected@file@percent }
\acronymused{dnn}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Neural Black-box Optimization (Neural-BO)}}{32}{algorithm.89}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:Neural-BO}{{1}{32}{Neural Black-box Optimization (Neural-BO)}{algorithm.89}{}}
\newlabel{line:calculate_var}{{3}{32}{Neural Black-box Optimization (Neural-BO)}{algorithm.89}{}}
\newlabel{line:train_NN}{{6}{32}{Neural Black-box Optimization (Neural-BO)}{algorithm.89}{}}
\newlabel{line:update_Ut}{{7}{32}{Neural Black-box Optimization (Neural-BO)}{algorithm.89}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces TrainNN}}{33}{algorithm.90}\protected@file@percent }
\newlabel{alg:train_NN}{{2}{33}{TrainNN}{algorithm.90}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Theoretical Analysis}{33}{section.91}\protected@file@percent }
\newlabel{section:neural-bo_regret_analysis}{{3.3}{33}{Theoretical Analysis}{section.91}{}}
\acronymused{ntk}
\acronymused{gp}
\acronymused{ts}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~3.3.1\else \numberline {3.3.1}Definition\fi \thmtformatoptarg {\blx@tocontentsinit {0}\cite {jacot2018neural}}}{33}{definition.93}\protected@file@percent }
\newlabel{def:NTK_matrix}{{3.3.1}{33}{\cite {jacot2018neural}}{definition.93}{}}
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~3.3.2\else \numberline {3.3.2}Assumption\fi }{33}{assumption.95}\protected@file@percent }
\newlabel{assumption:sufficient_exploration}{{3.3.2}{33}{}{assumption.95}{}}
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~3.3.3\else \numberline {3.3.3}Assumption\fi }{33}{assumption.97}\protected@file@percent }
\acronymused{rkhs}
\acronymused{ntk}
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~3.3.4\else \numberline {3.3.4}Assumption\fi }{33}{assumption.99}\protected@file@percent }
\newlabel{assumption:iid_noise}{{3.3.4}{33}{}{assumption.99}{}}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~3.3.5\else \numberline {3.3.5}Theorem\fi }{34}{theorem.101}\protected@file@percent }
\newlabel{theorem:neural-bo_main}{{3.3.5}{34}{}{theorem.101}{}}
\@writefile{loe}{\contentsline {remark}{\ifthmt@listswap Remark~3.3.6\else \numberline {3.3.6}Remark\fi }{34}{remark.103}\protected@file@percent }
\acronymused{ntk}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Proof of the Main Theorem}{34}{section.104}\protected@file@percent }
\newlabel{eqn:rkhs_lipschitz}{{3.1}{35}{Proof of the Main Theorem}{equation.105}{}}
\newlabel{proof:theorem_main}{{3.4}{35}{Proof of the Main Theorem}{equation.105}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Experiments}{37}{section.106}\protected@file@percent }
\newlabel{section:neural-bo_experiments}{{3.5}{37}{Experiments}{section.106}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Experimental Setup}{37}{subsection.107}\protected@file@percent }
\acronymused{gp}
\acronymused{rf}
\acronymused{dnn}
\acronymused{gp}
\acronymused{rf}
\acronymused{dnn}
\acronymused{rf}
\acronymused{dnn}
\acronymused{rf}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Synthetic Benchmarks}{38}{subsection.112}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The plots show the minimum true value observed after optimizing several synthetic functions over 2000 iterations of our proposed algorithm and 6 baselines. The dimension of each function is shown in the parenthesis.}}{38}{figure.caption.113}\protected@file@percent }
\newlabel{fig:neural-bo_synthetic}{{3.1}{38}{The plots show the minimum true value observed after optimizing several synthetic functions over 2000 iterations of our proposed algorithm and 6 baselines. The dimension of each function is shown in the parenthesis}{figure.caption.113}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces The p-values of KS-test "whether the data obtained from running our methods Neural-BO and all baselines are normally distributed".}}{39}{table.caption.114}\protected@file@percent }
\newlabel{tab:ks_test}{{3.1}{39}{The p-values of KS-test "whether the data obtained from running our methods Neural-BO and all baselines are normally distributed"}{table.caption.114}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces One-sided t-tests were employed to assess whether the baseline achieves a lower function value compared to our proposed method, Neural-BO. The null hypothesis $H_0: \mu _\text  {baseline} \le \mu _{\text  {Neural-BO}}$ and the alternative hypothesis: $H_a: \mu _\text  {baseline} > \mu _{\text  {Neural-BO}}$. The p-value corresponding to each test is provided as the first value in each cell. Moreover, to account for multiple hypotheses testing, the Benjamini-Hochberg correction was applied and is reported as the second value in each cell. In the outcome, a "T" indicates that the null hypothesis is rejected, whereas an "F" signifies that it is not rejected.}}{40}{table.caption.115}\protected@file@percent }
\newlabel{tab:t-test}{{3.2}{40}{One-sided t-tests were employed to assess whether the baseline achieves a lower function value compared to our proposed method, Neural-BO. The null hypothesis $H_0: \mu _\text {baseline} \le \mu _{\text {Neural-BO}}$ and the alternative hypothesis: $H_a: \mu _\text {baseline} > \mu _{\text {Neural-BO}}$. The p-value corresponding to each test is provided as the first value in each cell. Moreover, to account for multiple hypotheses testing, the Benjamini-Hochberg correction was applied and is reported as the second value in each cell. In the outcome, a "T" indicates that the null hypothesis is rejected, whereas an "F" signifies that it is not rejected}{table.caption.115}{}}
\newlabel{fig:coco_2d}{{3.2a}{41}{2D}{figure.caption.117}{}}
\newlabel{sub@fig:coco_2d}{{a}{41}{2D}{figure.caption.117}{}}
\newlabel{fig:coco_3d}{{3.2b}{41}{3D}{figure.caption.117}{}}
\newlabel{sub@fig:coco_3d}{{b}{41}{3D}{figure.caption.117}{}}
\newlabel{fig:coco_5d}{{3.2c}{41}{5D}{figure.caption.117}{}}
\newlabel{sub@fig:coco_5d}{{c}{41}{5D}{figure.caption.117}{}}
\newlabel{fig:coco_10d}{{3.2d}{41}{10D}{figure.caption.117}{}}
\newlabel{sub@fig:coco_10d}{{d}{41}{10D}{figure.caption.117}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The results of benchmarking our Neural-BO and the baselines with COCO framework on 24 BBOB noiseless objective functions with four different dimensions \{2,3,5,10\}.}}{41}{figure.caption.117}\protected@file@percent }
\newlabel{fig:coco}{{3.2}{41}{The results of benchmarking our Neural-BO and the baselines with COCO framework on 24 BBOB noiseless objective functions with four different dimensions \{2,3,5,10\}}{figure.caption.117}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Real-world Applications}{42}{subsection.118}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.1}Designing Sensitive Samples for Detection of Model Tampering}{42}{subsubsection.119}\protected@file@percent }
\newlabel{section:neural-bo_sensitive_samples}{{3.5.3.1}{42}{Designing Sensitive Samples for Detection of Model Tampering}{subsubsection.119}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The plot shows the \textbf  {detection rates} corresponding to the number of samples on the MNIST dataset. The larger the number of sensitive samples, the higher the detection rate. As shown in the figure, Neural-BO can generate sensitive samples that achieve nearly 90\% of the detection rate with at least 8 samples.}}{42}{figure.caption.120}\protected@file@percent }
\newlabel{fig:neural-bo_sensitive_sample}{{3.3}{42}{The plot shows the \textbf {detection rates} corresponding to the number of samples on the MNIST dataset. The larger the number of sensitive samples, the higher the detection rate. As shown in the figure, Neural-BO can generate sensitive samples that achieve nearly 90\% of the detection rate with at least 8 samples}{figure.caption.120}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.2}Unknown target document retrieval}{43}{subsubsection.121}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces We search for the most related document for a specified target document in \textbf  {Amazon product reviews} dataset and report the maximum \textbf  {hierachical F1 score} found by all baselines. All methods show similar behaviour and Neural-BO performs comparably and much better than GP-based baselines.}}{43}{figure.caption.122}\protected@file@percent }
\newlabel{fig:text}{{3.4}{43}{We search for the most related document for a specified target document in \textbf {Amazon product reviews} dataset and report the maximum \textbf {hierachical F1 score} found by all baselines. All methods show similar behaviour and Neural-BO performs comparably and much better than GP-based baselines}{figure.caption.122}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.3}Optimizing control parameters for robot pushing}{44}{subsubsection.124}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces  Optimization results for control parameters of 14D robot pushing problem. The X-axis shows iterations, and the y-axis shows the median of the best reward obtained.}}{44}{figure.caption.125}\protected@file@percent }
\newlabel{fig:robot_14D}{{3.5}{44}{Optimization results for control parameters of 14D robot pushing problem. The X-axis shows iterations, and the y-axis shows the median of the best reward obtained}{figure.caption.125}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion}{45}{section.126}\protected@file@percent }
\acronymused{bo}
\acronymused{dnn}
\acronymused{gp}
\@setckpt{Chapters/neural-bo}{
\setcounter{page}{46}
\setcounter{equation}{1}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{6}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{2}
\setcounter{LT@tables}{1}
\setcounter{LT@chunks}{1}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{106}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{1}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALG@line}{4}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{parentequation}{0}
\setcounter{thmt@dummyctr}{7}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{theorem}{0}
\setcounter{sublemma}{0}
\setcounter{section@level}{1}
\setcounter{Item}{7}
\setcounter{Hfootnote}{6}
\setcounter{bookmark@seq@number}{70}
}
