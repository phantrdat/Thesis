\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks}{37}{chapter.122}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:neural-bo}{{3}{37}{Neural-BO: A Black-box Optimization Algorithm using Deep Neural Networks}{chapter.122}{}}
\acronymused{gp}
\acronymused{bo}
\acronymused{gp}
\acronymused{bo}
\acronymused{gp}
\acronymused{gp}
\acronymused{dnn}
\acronymused{gp}
\acronymused{dnn}
\acronymused{bnn}
\acronymused{dnn}
\acronymused{bnn}
\acronymused{ntk}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Problem Setting}{38}{section.123}\protected@file@percent }
\newlabel{section:neural-bo_problem_setting}{{3.1}{38}{Problem Setting}{section.123}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Proposed Neural-BO Method}{38}{section.124}\protected@file@percent }
\newlabel{section:neural-bo_proposed_method}{{3.2}{38}{Proposed Neural-BO Method}{section.124}{}}
\acronymused{dnn}
\acronymused{bo}
\acronymused{gp}
\AC@undonewlabel{acro:relu}
\newlabel{acro:relu}{{3.2}{38}{Proposed Neural-BO Method}{section.124}{}}
\acronymused{relu}
\@writefile{toc}{\contentsline {paragraph}{Discussion}{39}{section*.125}\protected@file@percent }
\acronymused{dnn}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Neural Black-box Optimization (Neural-BO)}}{39}{algorithm.126}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{alg:Neural-BO}{{1}{39}{Neural Black-box Optimization (Neural-BO)}{algorithm.126}{}}
\newlabel{line:calculate_var}{{3}{39}{Neural Black-box Optimization (Neural-BO)}{algorithm.126}{}}
\newlabel{line:train_NN}{{6}{39}{Neural Black-box Optimization (Neural-BO)}{algorithm.126}{}}
\newlabel{line:update_Ut}{{7}{39}{Neural Black-box Optimization (Neural-BO)}{algorithm.126}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces TrainNN}}{40}{algorithm.127}\protected@file@percent }
\newlabel{alg:train_NN}{{2}{40}{TrainNN}{algorithm.127}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Theoretical Analysis}{40}{section.128}\protected@file@percent }
\newlabel{section:neural-bo_regret_analysis}{{3.3}{40}{Theoretical Analysis}{section.128}{}}
\acronymused{ntk}
\acronymused{gp}
\acronymused{ts}
\@writefile{loe}{\addvspace {10\p@ }}
\@writefile{loe}{\contentsline {definition}{\ifthmt@listswap Definition~3.3.1\else \numberline {3.3.1}Definition\fi \thmtformatoptarg {\blx@tocontentsinit {0}\cite {jacot2018neural}}}{40}{definition.130}\protected@file@percent }
\newlabel{def:NTK_matrix}{{3.3.1}{40}{\cite {jacot2018neural}}{definition.130}{}}
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~3.3.2\else \numberline {3.3.2}Assumption\fi }{40}{assumption.132}\protected@file@percent }
\newlabel{assumption:sufficient_exploration}{{3.3.2}{40}{}{assumption.132}{}}
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~3.3.3\else \numberline {3.3.3}Assumption\fi }{40}{assumption.134}\protected@file@percent }
\acronymused{rkhs}
\acronymused{ntk}
\@writefile{loe}{\contentsline {assumption}{\ifthmt@listswap Assumption~3.3.4\else \numberline {3.3.4}Assumption\fi }{40}{assumption.136}\protected@file@percent }
\newlabel{assumption:iid_noise}{{3.3.4}{40}{}{assumption.136}{}}
\@writefile{loe}{\contentsline {theorem}{\ifthmt@listswap Theorem~3.3.5\else \numberline {3.3.5}Theorem\fi }{41}{theorem.138}\protected@file@percent }
\newlabel{theorem:neural-bo_main}{{3.3.5}{41}{}{theorem.138}{}}
\@writefile{loe}{\contentsline {remark}{\ifthmt@listswap Remark~3.3.6\else \numberline {3.3.6}Remark\fi }{41}{remark.140}\protected@file@percent }
\acronymused{ntk}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Proof of the Main Theorem}{41}{section.141}\protected@file@percent }
\newlabel{eqn:rkhs_lipschitz}{{3.1}{42}{Proof of the Main Theorem}{equation.142}{}}
\newlabel{proof:theorem_main}{{3.4}{42}{Proof of the Main Theorem}{equation.142}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Experiments}{44}{section.143}\protected@file@percent }
\newlabel{section:neural-bo_experiments}{{3.5}{44}{Experiments}{section.143}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}Experimental Setup}{44}{subsection.144}\protected@file@percent }
\acronymused{gp}
\acronymused{rf}
\acronymused{dnn}
\acronymused{gp}
\acronymused{rf}
\acronymused{dnn}
\acronymused{rf}
\acronymused{dnn}
\acronymused{rf}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Synthetic Benchmarks}{45}{subsection.149}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces The plots show the minimum true value observed after optimizing several synthetic functions over 2000 iterations of our proposed algorithm and 6 baselines. The dimension of each function is shown in the parenthesis.}}{45}{figure.caption.150}\protected@file@percent }
\newlabel{fig:neural-bo_synthetic}{{3.1}{45}{The plots show the minimum true value observed after optimizing several synthetic functions over 2000 iterations of our proposed algorithm and 6 baselines. The dimension of each function is shown in the parenthesis}{figure.caption.150}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces The p-values of KS-test "whether the data obtained from running our methods Neural-BO and all baselines are normally distributed".}}{46}{table.caption.151}\protected@file@percent }
\newlabel{tab:ks_test}{{3.1}{46}{The p-values of KS-test "whether the data obtained from running our methods Neural-BO and all baselines are normally distributed"}{table.caption.151}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces One-sided t-tests were employed to assess whether the baseline achieves a lower function value compared to our proposed method, Neural-BO. The null hypothesis $H_0: \mu _\text  {baseline} \le \mu _{\text  {Neural-BO}}$ and the alternative hypothesis: $H_a: \mu _\text  {baseline} > \mu _{\text  {Neural-BO}}$. The p-value corresponding to each test is provided as the first value in each cell. Moreover, to account for multiple hypotheses testing, the Benjamini-Hochberg correction was applied and is reported as the second value in each cell. In the outcome, a "T" indicates that the null hypothesis is rejected, whereas an "F" signifies that it is not rejected.}}{47}{table.caption.152}\protected@file@percent }
\newlabel{tab:t-test}{{3.2}{47}{One-sided t-tests were employed to assess whether the baseline achieves a lower function value compared to our proposed method, Neural-BO. The null hypothesis $H_0: \mu _\text {baseline} \le \mu _{\text {Neural-BO}}$ and the alternative hypothesis: $H_a: \mu _\text {baseline} > \mu _{\text {Neural-BO}}$. The p-value corresponding to each test is provided as the first value in each cell. Moreover, to account for multiple hypotheses testing, the Benjamini-Hochberg correction was applied and is reported as the second value in each cell. In the outcome, a "T" indicates that the null hypothesis is rejected, whereas an "F" signifies that it is not rejected}{table.caption.152}{}}
\newlabel{fig:coco_2d}{{3.2a}{48}{2D}{figure.caption.154}{}}
\newlabel{sub@fig:coco_2d}{{a}{48}{2D}{figure.caption.154}{}}
\newlabel{fig:coco_3d}{{3.2b}{48}{3D}{figure.caption.154}{}}
\newlabel{sub@fig:coco_3d}{{b}{48}{3D}{figure.caption.154}{}}
\newlabel{fig:coco_5d}{{3.2c}{48}{5D}{figure.caption.154}{}}
\newlabel{sub@fig:coco_5d}{{c}{48}{5D}{figure.caption.154}{}}
\newlabel{fig:coco_10d}{{3.2d}{48}{10D}{figure.caption.154}{}}
\newlabel{sub@fig:coco_10d}{{d}{48}{10D}{figure.caption.154}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The results of benchmarking our Neural-BO and the baselines with COCO framework on 24 BBOB noiseless objective functions with four different dimensions \{2,3,5,10\}.}}{48}{figure.caption.154}\protected@file@percent }
\newlabel{fig:coco}{{3.2}{48}{The results of benchmarking our Neural-BO and the baselines with COCO framework on 24 BBOB noiseless objective functions with four different dimensions \{2,3,5,10\}}{figure.caption.154}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Real-world Applications}{49}{subsection.155}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.1}Designing Sensitive Samples for Detection of Model Tampering}{49}{subsubsection.156}\protected@file@percent }
\newlabel{section:neural-bo_sensitive_samples}{{3.5.3.1}{49}{Designing Sensitive Samples for Detection of Model Tampering}{subsubsection.156}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The plot shows the \textbf  {detection rates} corresponding to the number of samples on the MNIST dataset. The larger the number of sensitive samples, the higher the detection rate. As shown in the figure, Neural-BO can generate sensitive samples that achieve nearly 90\% of the detection rate with at least 8 samples.}}{49}{figure.caption.157}\protected@file@percent }
\newlabel{fig:neural-bo_sensitive_sample}{{3.3}{49}{The plot shows the \textbf {detection rates} corresponding to the number of samples on the MNIST dataset. The larger the number of sensitive samples, the higher the detection rate. As shown in the figure, Neural-BO can generate sensitive samples that achieve nearly 90\% of the detection rate with at least 8 samples}{figure.caption.157}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.2}Unknown target document retrieval}{50}{subsubsection.158}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces We search for the most related document for a specified target document in \textbf  {Amazon product reviews} dataset and report the maximum \textbf  {hierachical F1 score} found by all baselines. All methods show similar behaviour and Neural-BO performs comparably and much better than GP-based baselines.}}{50}{figure.caption.159}\protected@file@percent }
\newlabel{fig:text}{{3.4}{50}{We search for the most related document for a specified target document in \textbf {Amazon product reviews} dataset and report the maximum \textbf {hierachical F1 score} found by all baselines. All methods show similar behaviour and Neural-BO performs comparably and much better than GP-based baselines}{figure.caption.159}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.3.3}Optimizing control parameters for robot pushing}{51}{subsubsection.161}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces  Optimization results for control parameters of 14D robot pushing problem. The X-axis shows iterations, and the y-axis shows the median of the best reward obtained.}}{51}{figure.caption.162}\protected@file@percent }
\newlabel{fig:robot_14D}{{3.5}{51}{Optimization results for control parameters of 14D robot pushing problem. The X-axis shows iterations, and the y-axis shows the median of the best reward obtained}{figure.caption.162}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion}{52}{section.163}\protected@file@percent }
\acronymused{bo}
\acronymused{dnn}
\acronymused{gp}
\@setckpt{Chapters/neural-bo}{
\setcounter{page}{53}
\setcounter{equation}{1}
\setcounter{enumi}{2}
\setcounter{enumii}{5}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{6}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{5}
\setcounter{table}{2}
\setcounter{LT@tables}{1}
\setcounter{LT@chunks}{1}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{129}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{0}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{1}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{2}
\setcounter{ALG@line}{4}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{parentequation}{0}
\setcounter{thmt@dummyctr}{7}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{theorem}{0}
\setcounter{sublemma}{0}
\setcounter{section@level}{1}
\setcounter{Item}{21}
\setcounter{Hfootnote}{6}
\setcounter{bookmark@seq@number}{71}
}
