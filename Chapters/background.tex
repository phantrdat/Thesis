\chapter{Background} % Main chapter title

\label{chap:background} 

% Define some commands to keep the formatting separated from the content 
This chapter provides a structured exploration of black-box optimization, beginning with the basic mathematical concepts necessary for understanding the subject. Section \ref{section:math_backgrounds} introduces the fundamental mathematical tools and principles that support optimization theory, ensuring a solid foundation for later discussions. Section \ref{section:optimization_introduction} provides a general overview of optimization, distinguishing between local and global approaches and highlighting their respective roles and applications.

In Section \ref{section:black_box_optimization}, we shift our focus to black-box optimization, examining the challenges posed by functions that are expensive to evaluate or lack explicit mathematical representation. Section \ref{section:bo_unknown_constraints} extends this discussion by addressing problems with unknown constraints, emphasizing strategies to effectively explore feasible regions in such complex scenarios. Section \ref{section:bo_physics} explores how incorporating physics-based knowledge into optimization frameworks can improve both solution accuracy and computational efficiency. Finally, in Section \ref{section:bo_further_research}, we conclude the chapter by outlining potential future research directions, focusing on unresolved challenges and emerging opportunities in black-box optimization.

\section{Essential Mathematics Backgrounds}
\label{section:math_backgrounds}
\subsection{Gaussian and Sub-Gaussian Random Variables}
\subsubsection{Gaussian Random Variables}  
Gaussian random variables, also known as normal random variables, are fundamental in probability theory and statistics. A Gaussian random variable \( X \) is fully characterized by its mean \( \mu \) and variance \( \sigma^2 \), and its probability density function (PDF) is given by:  
\[
f_X(x) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right), \quad x \in \mathbb{R}.
\]  
The tails of a Gaussian distribution decay exponentially, meaning the probability of large deviations from the mean diminishes rapidly. Specifically, for \( X \sim \mathcal{N}(\mu, \sigma^2) \), the tail probability can be bounded as:  
\[
\mathbb{P}(|X - \mu| > t) \leq 2 \exp\left(-\frac{t^2}{2\sigma^2}\right), \quad t > 0.
\]  

This exponential decay of tail probabilities is a key property that makes Gaussian distributions central to statistical theory and concentration inequalities. However, many random variables in practice exhibit similar tail behaviors without necessarily following the exact Gaussian distribution. This motivates the broader class of \textit{sub-Gaussian random variables}.  

\subsubsection{Sub-Gaussian Random Variables}  
A random variable \( X \) is sub-Gaussian if its tail probabilities decay at least as fast as those of a Gaussian random variable. Intuitively, sub-Gaussian random variables are those whose deviations from their mean are tightly controlled, similar to or better than the Gaussian case.  

Formally, \( X \) is sub-Gaussian if there exists a constant \( \sigma > 0 \) such that for all \( t > 0 \):  
\[
\mathbb{P}(|X - \mathbb{E}[X]| > t) \leq 2 \exp\left(-\frac{t^2}{2\sigma^2}\right).
\]  
Here, \( \sigma^2 \) is called the \textit{sub-Gaussian variance parameter}, and it provides a measure of the "spread" of the random variable, analogous to the variance in a Gaussian distribution.  

An equivalent definition involves the moment generating function (MGF). A random variable \( X \) is sub-Gaussian if there exists a constant \( \sigma > 0 \) such that:  
\[
\mathbb{E}[\exp(\lambda(X - \mathbb{E}[X]))] \leq \exp\left(\frac{\lambda^2 \sigma^2}{2}\right), \quad \forall \lambda \in \mathbb{R}.
\]  

Many common random variables exhibit sub-Gaussian behavior, including Gaussian, bounded, and discrete random variables. Below are some illustrative examples:  
\begin{itemize}
    \item \textbf{Gaussian Random Variables:} Any Gaussian random variable \( X \sim \mathcal{N}(\mu, \sigma^2) \) is inherently sub-Gaussian, with \( \sigma^2 \) as its variance.  
    \item \textbf{Bounded Random Variables:} If \( X \) is a bounded random variable, i.e., \( |X| \leq B \) almost surely, then \( X \) is sub-Gaussian with \( \sigma \leq B \).  
    \item \textbf{Rademacher Random Variables:} A Rademacher random variable, which takes values \( \pm 1 \) with equal probability, is sub-Gaussian with \( \sigma = 1 \).  
\end{itemize}  

Sub-Gaussian random variables exhibit several important properties that make them useful in probabilistic analyses, particularly when handling extreme deviations and ensuring concentration of measure. These properties include:  
\begin{itemize}
    \item \textbf{Tight Tail Bounds:} Sub-Gaussian random variables satisfy exponential tail bounds, making them useful in scenarios where extreme deviations must be controlled.  
    \item \textbf{Closed under Affine Transformations:} If \( X \) is sub-Gaussian, then \( aX + b \) is also sub-Gaussian, with the sub-Gaussian parameter scaled by \( |a| \).  
    \item \textbf{Sum of Independent Sub-Gaussians:} If \( X_1, X_2, \dots, X_n \) are independent sub-Gaussian random variables, their sum \( S_n = \sum_{i=1}^n X_i \) is also sub-Gaussian, with the variance parameter satisfying \( \sigma^2_{S_n} = \sum_{i=1}^n \sigma_i^2 \).  
\end{itemize} 

While sub-Gaussian random variables provide valuable tools for modeling uncertainties in single values, many real-world problems cannot be modeled with single values, as these problems often deal with the uncertainty over entire functions. Therefore, it becomes necessary to capture the dependencies between the values of the function at different locations or times, which implies a need for working with joint distributions. The key challenge arises when dealing with functions, which can take values at infinitely many points, thus requiring a notion of probability distributions over functions. In the next section,  we introduce \acf{gp}, which is the natural extension of the normal distribution from the case of individual random variables to function spaces.

\subsection{Gaussian Process}
\label{section:gaussian_process}
A \acf{gp} is a finite collection of normally distributed random variables with each having a multivariate normal distribution. \ac{gp} offers a prior distribution over smooth functions and are completely specified by a mean function, $\mu(\mathbf{x})$ and covariance function, $k(\mathbf{x}, \mathbf{x}^\prime)$. We have \begin{equation}
    \label{def:GP}
    f({\bf x})\sim\mathrm{GP}(\mu({\bf x}),k({\bf x},{\bf x}^\prime))
\end{equation}
where the mean and variance of a normal distribution is returned as the value of $f(\mathbf{x})$ for an arbitrary $\mathbf{x}$. Without loss in generality, $f(\mathbf{x})$ values can be assumed as a realization of random variables with prior mean as zero thus making the Gaussian process fully specified by the covariance matrix \citep{rasmussen2006gaussian}. There are many popular covariance functions including Mat\'ern kernel, Linear kernel, Squared Exponential kernel. These kernels will be briefly introduced in the next section.

% In that the Squared Exponential (SE) kernel is a popular choice.

% \begin{equation}
%      \left.k({\bf x},{\bf x}^{\prime}) = \exp (-\frac{1}{2 \ell^{2}} \norm{\bf x - \bf x^{\prime}}^{2}) \right.
% \end{equation}
% In the above kernel, $\ell$ is a length scale parameter related to the smoothness of the function. When $\mathbf{x}_i$ and $\mathbf{x}_j$
% values get close together, this function returns high value and else it returns value close to 0.
We assume that function values ${\mathbf{y}}_{1:t} = f(\mathbf{x}_{1:t})+\epsilon_{1:t}$ corresponding to the initial points $\mathbf{x}_{1:t}$ are sampled
from the prior Gaussian process. Collectively we denote the observations as $\mathcal{D}_{1:t} = \{\mathbf{x}_{1:t}, \mathbf{y}_{1:t}\}$. The function
values $f(\mathbf{x}_{1:t})$ follow a multivariate Gaussian distribution $\mathcal{N}(0, \mathbf{K}_t)$, where
\[ \mathbf{K}_t = \begin{bmatrix} 
    k(\mathbf{x}_1, \mathbf{x}_1) & \dots  & k(\mathbf{x}_1, \mathbf{x}_t)\\
    \vdots & \ddots & \vdots\\
    k(\mathbf{x}_t, \mathbf{x}_1) & \dots  & k(\mathbf{x}_t, \mathbf{x}_t)
    \end{bmatrix}
\]
Given a new point $\mathbf{x}_{t+1}$, then $\mathbf{x}_{t+1},  y_{1:t}$ and $f(\mathbf{x}_{t+1})$ are jointly Gaussian. By the properties of Gaussian process we
can write

\[
\begin{pmatrix}
\mathbf{y}_{1:t} \\
f(\mathbf{x}_{t+1})
\end{pmatrix} \sim \mathcal{N}\left(0, \begin{bmatrix}
\mathbf{K}_t & \mathbf{k}_{1:t} \\
\mathbf{k}_{1:t}^\top & k(\mathbf{x}_{t+1}, \mathbf{x}_{t+1})
\end{bmatrix} \right), 
\]
where $\mathbf{k}_{1:t} = \begin{bmatrix}
k(\mathbf{x}_{t+1}, \mathbf{x}_1) & k(\mathbf{x}_{t+1}, \mathbf{x}_2) & \dots & k(\mathbf{x}_{t+1}, \mathbf{x}_t)
\end{bmatrix}$.

Then using Sherman-Morrison-Woodbury formula in \citet{rasmussen2006gaussian}, the predictive distribution is:
\[ \mathbb{P}(y_{t+1} \lvert \mathcal{D}_{1:t}, \mathbf{x}_{t+1})= \mathcal{N}(\mu_t(\mathbf{x}_{t+1}), \sigma_t^2(\mathbf{x}_{t+1})), 
\]
where the predictive mean is $\mu_t(\mathbf{x}_{t+1}) = \mathbf{k}_{1:t}^\top \mathbf{K}_t^{-1} \mathbf{y}_{1:t}$ and variance $\sigma_{t}^{2}({\bf x}_{t+1}) = k({\bf x}_{t+1},{\bf x}_{t+1})-{\bf k}_{1:t}^{\top} \mathbf{K}_t^{-1}{\bf k}_{1:t}$. 

\subsection{Kernels}
\label{section:kernels}
In the context of \acfp{gp}, kernels (also known as covariance functions) play a crucial role in defining the prior belief about the function being modeled. A kernel, denoted as $k(\mathbf{x}, \mathbf{x}^\prime)$, quantifies the similarity or correlation between function values at two input locations, $\mathbf{x}$ and $\mathbf{x}^\prime$. It is symmetric, positive semi-definite, and it defines the properties of the functions sampled from the \ac{gp} prior. These properties ensure that the resulting covariance matrix constructed using this kernel will be valid (positive semi-definite), which is necessary for the Gaussian Process to be well-defined.

The choice of kernel is a critical aspect of \ac{gp} modeling, as it directly impacts the smoothness, complexity, and overall behavior of the functions that the model is capable of representing. Different kernels encode different assumptions about the underlying function we are attempting to model, and it is crucial to carefully consider these assumptions when choosing a kernel for a specific task. From the perspective of Bayesian inference, the kernel can be seen as the main source of prior information about the properties of the functions we are modeling. Therefore, it is essential to choose a kernel that is appropriate for the structure and characteristics that one might expect from the function. We now consider some common types of kernels, each with its own assumptions and characteristics:

\paragraph{Linear Kernel:}
The linear kernel is defined as:
\[k(\mathbf{x}, \mathbf{x}^\prime) = \mathbf{x}^T \mathbf{x}^\prime\]
where $\mathbf{x}$ and $\mathbf{x}^\prime$ are vectors in the input space. This kernel models the assumption that the function values are linearly related to the inputs. The resulting functions are linear combinations of the input variables. This kernel is useful when there is a reason to assume that the underlying data has a linear structure, or when working with high-dimensional data where the kernel has less sensitivity to changes in the input. However, this kernel is often too restrictive for complex functions due to its lack of flexibility and can't capture non-linear patterns. 

\paragraph{Squared Exponential (RBF) Kernel}

The squared exponential kernel, also known as the Radial Basis Function (RBF) kernel, is defined as:
\[ k(\mathbf{x}, \mathbf{x}^\prime) = \exp\left(-\frac{\norm{\mathbf{x} - \mathbf{x}^\prime}^2}{2\ell^2}\right),\]
where $\ell$ is the length-scale parameter. The RBF kernel assumes that the function values are highly correlated when the inputs are close to each other. The parameter $\ell$ controls the "reach" of the correlation; a larger $\ell$ implies that the function values are correlated even when the inputs are farther apart. This kernel is associated with infinitely smooth functions, which implies that the functions represented with this kernel do not have sharp changes or rapid variations. The RBF kernel is widely used for its flexibility and its ability to model non-linear functions, and also because its parameter controls the range of smoothness of the represented functions. However, its isotropic nature might make it not ideal in scenarios where the different dimensions have different degrees of correlation.

\paragraph{Mat\'ern Kernel}

The Mat\'ern kernel is a more general kernel and is defined as:
\[ k(\mathbf{x}, \mathbf{x}^\prime) = \frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2\nu}\norm{\mathbf{x}-\mathbf{x}^\prime}}{\ell}\right)^\nu K_\nu\left(\frac{\sqrt{2\nu}\norm{\mathbf{x}-\mathbf{x}^\prime}}{\ell}\right)\]
where $\nu > 0$ is the smoothness parameter, $\ell$ is the length-scale parameter, and $K_\nu$ is the modified Bessel function of the second kind. The Mat\'ern kernel is a generalization of the RBF kernel, which is obtained in the limit of large $\nu$.  This kernel allows for controlling the smoothness of the underlying function, providing a continuous range of possibilities. The smoothness of functions associated with the Mat\'ern kernel is determined by the parameter $\nu$. A low $\nu$ implies less smooth functions, whereas, as $\nu$ increases, the functions become increasingly smoother. This parameter also has an impact on the convergence of Bayesian Optimization algorithms. Mat\'ern kernels with different smoothness parameters have also been found to have a crucial effect in the generalization capacity of Gaussian Process models, as it reflects the trade-off between modeling data and having a smooth prior \citep{rasmussen2006gaussian}. The Mat\'ern kernel allows the flexibility of the RBF kernel, but includes an additional parameter that enables one to control the smoothness of the modeled function. This parameter is not present in the case of RBF kernels.

The choice of the kernel is not merely a matter of convenience; it directly influences the properties of the function space over which the Gaussian process operates. Selecting an appropriate kernel demands a careful understanding of the underlying assumptions of each kernel type and considering the characteristics of the function to be modeled. Kernels enable us to encode our prior beliefs about the function, and hence, proper selection is crucial for the efficiency and generalization capability of our Gaussian Process model. We have presented three common kernels that are often used in practice, and their differences highlight the importance of selecting the appropriate kernel for the task. The theoretical properties of these kernels can be found in the literature. Choosing between them requires an understanding of the properties of the data and the functions to be modeled.
\subsection{Maximum Information Gain}
\label{section:MIG}
In the context of machine learning and information theory, \ac{mig} is a principle used to select features, queries, or actions that yield the highest amount of information about an unknown variable of interest. The concept of information gain is rooted in Shannon’s entropy theory and measures the reduction in uncertainty about a random variable after observing another variable. 

\subsubsection{Entropy}
\label{section:entropy}
Entropy is a measure of the uncertainty or unpredictability in a random variable. For a discrete random variable $X$ with possible outcomes $\mathbf{x} \in \mathcal{X}$ and probability distribution $p(X)$, the entropy $H(X)$ is defined as:
\[
H(X) = - \sum_{\mathbf{x} \in \mathcal{X}} p(\mathbf{x}) \log p(\mathbf{x}).
\]
This entropy value represents the average number of bits required to encode information about the variable $X$. When we consider a second random variable $Y$, we can examine how much observing $Y$ reduces uncertainty about $X$, which is the basis for defining information gain.

The \textit{conditional entropy} of $X$ given $Y$, denoted $H(X|Y)$, measures the remaining uncertainty about $X$ after observing $Y$. It is defined as:
\[
H(X|Y) = - \sum_{\mathbf{y} \in \mathcal{Y}} p(\mathbf{y}) \sum_{\mathbf{x} \in \mathcal{X}} p(\mathbf{x} | \mathbf{y}) \log p(\mathbf{x} | \mathbf{y}).
\]
The conditional entropy quantifies the uncertainty in $X$ given that we know $Y$. If observing $Y$ provides information about $X$, we expect $H(X|Y) < H(X)$.

\subsubsection{Mutual Information}

The \textit{mutual information} $IG(X; Y)$ between two random variables $X$ and $Y$ is defined as the reduction in entropy of $X$ after observing $Y$, and is given by:
\[
IG(X; Y) = H(X) - H(X|Y).
\]
This quantity represents the average reduction in uncertainty about $X$ provided by knowledge of $Y$. Alternatively, it can also be expressed in terms of the mutual information $I(X; Y)$ between $X$ and $Y$:
\[
IG(X; Y) = I(X; Y),
\]
where mutual information is defined as:
\[
I(X; Y) = \sum_{\mathbf{x} \in \mathcal{X}} \sum_{\mathbf{y} \in \mathcal{Y}} p(\mathbf{x}, \mathbf{y}) \log \frac{p(\mathbf{x}, \mathbf{y})}{p(\mathbf{x}) p(\mathbf{y})}.
\]
Mutual information is symmetric and non-negative, i.e., $I(X; Y) = I(Y; X) \geq 0$, and measures the amount of shared information between $X$ and $Y$.

While the concept of Mutual Information generally quantifies the reduction in uncertainty about one random variable when observing another, its application becomes specialized within the context of \acp{gp}. Specifically, when using a \ac{gp} to learn an unknown function $f$, the information gain is redefined as the mutual information between the noisy observations (the function values at chosen points) and the true, underlying function values $f$. This specialization allows us to analyze the effectiveness of our \ac{gp} learning process based on the information provided by our observed data. 
\subsubsection{Maximum Information Gain in GP regression}

Assume we have a \ac{gp} prior over \( f \) with mean function \( \mu(\mathbf{x}) \) and covariance \( k(\mathbf{x}, \mathbf{x}^\prime) \). After $t$ steps, the model receives an input sequence $\mathcal{X}_t = (\mathbf{x}_1, \mathbf{x}_2, \dots  \mathbf{x}_t)$ and observes noisy rewards $\mathbf{y}_t = (y_1, y_2, \dots, y_t)$. The \emph{information gain} at step $t$, quantifies the reduction in uncertainty about $f$ after observing $\mathbf{y}_t$, defined as the mutual information between  $\mathbf{y}_t$ and $f$:
\[
I(\mathbf{y}_t; f) \coloneqq  H(\mathbf{y}_t) - H(\mathbf{y}_t \rvert f), 
\]
where $H$ denotes the entropy. To obtain the closed-form expression of information gain, one needs to introduce a GP model where $f$ is assumed to be a zero mean GP indexed on $\mathcal X$ with kernel $k$. Let $\mathbf{f}_t = [\left(f(\mathbf{x}_1), f(\mathbf{x}_1), \dots, f(\mathbf{x}_t)] \right)$ be the corresponding true function values.  From \citet{cover1999elements}, the mutual information between two multivariate Gaussian random variables is: 
\[ I(\mathbf{y}_t; f) = I(\mathbf{y}_t; \mathbf{f}_t) = \frac{1}{2} \log \det (\mathbf{I}_t + \lambda^{-1}\mathbf{K}_t), \]
where $\lambda > 0$ is a regularization parameter, $\mathbf{K}_t$ is the covariance kernel matrix of the points \( \{\mathbf{x}_1, \dots, \mathbf{x}_n\} \),
and \( \mathbf{I}_t \) is the identity matrix.


The Information Gain obtained is highly dependent on the specific locations where observations are made. To address this variability, the \acf{mig} after observing $t$ data points, represented as $\gamma_t$, is introduced as an upper bound. Instead of focusing on a particular point selection, \ac{mig} quantifies the maximum possible information that could be gained from any set of $t$ points in the input space. \ac{mig} provides an input-independent and kernel-specific bound on the information that can be acquired:

\[ \gamma_t := \max_{\mathcal{A} \subset \mathcal{X}, \lvert \mathcal{A} \rvert = t} I(\mathbf{y}_A; \mathbf{f}_A),\]

As mentioned above, $\mathcal{A}$ represents a set of $t$ input points in the input space $\mathcal{X}$, $\mathbf{y}_\mathcal{A}$ denotes the noisy observations of the objective function $f$ at these points, and $\mathbf{f}_\mathcal{A}$ represents the corresponding function values \textit{without} noise. $I(\mathbf{y}_\mathcal{A}; \mathbf{f}_\mathcal{A})$ is the mutual information between the observed data $\mathbf{y}_\mathcal{A}$ and the underlying function values $\mathbf{f}_\mathcal{A}$. Basically, the \ac{mig} quantifies the maximum amount of information that \textit{any} set of $t$ points could possibly reveal about the unknown objective function $f$. Specifically, the \ac{mig} serves as an upper bound on the information obtained by any selection of $t$ points. This means that even in the worst-case scenario (i.e., the scenario where we choose the least informative set of points within the optimization algorithm), the amount of information gained will never surpass the \ac{mig}. The work by \citet{srinivas2009gaussian} provides crucial kernel-specific bounds on the \ac{mig}. Specifically, they analyzed three common kernels: Linear, RBF, and Mat\'ern introduced in Section \ref{section:kernels}. The bounds on \ac{mig} for these kernels are:

\begin{itemize}
    \item \textbf{Linear Kernel:}
    The MIG is bounded by
    \[ \gamma_t = \mathcal{O}(d \log t),\]
    where $d$ is the input dimension.

    \item \textbf{Squared Exponential (RBF) Kernel:} The \ac{mig} for RBF kernel is bounded by:
    \[ \gamma_t = \mathcal{O}((\log t)^{d+1}). \]


    \item \textbf{Mat\'ern Kernel:} For a specific smoothness parameter $\nu$ of Mat\'ern kernel, the \ac{mig} has a bound:
     \[ \gamma_t = \mathcal{O} \left( t^\frac{d(d+1)}{2\nu+ d(d+1)} \log t \right). \]
    
\end{itemize}





\subsection{\acf{rkhs}}
\label{background:rkhs}
\acp{rkhs} provide a rigorous framework for dealing with functions in high-dimensional spaces using kernel methods, which are central in machine learning, functional analysis, and statistics. Formally, \acp{rkhs} are Hilbert spaces associated with a positive definite kernel, allowing efficient manipulation of functions via inner products and enabling regularization in infinite-dimensional settings.

\subsubsection{Hilbert Spaces and Inner Products}

A \textit{Hilbert space} $\mathcal{H}$ is a complete vector space equipped with an inner product $\langle \cdot, \cdot \rangle_{\mathcal{H}}$ such that every Cauchy sequence in $\mathcal{H}$ converges in $\mathcal{H}$. Completeness is crucial for ensuring that limits of function sequences (e.g., solutions to optimization problems) lie within the space.

For an inner product space $\mathcal{H}$, the norm induced by the inner product is given by:
\[
\norm{f}_{\mathcal{H}} = \sqrt{\langle f, f \rangle_{\mathcal{H}}}.
\]

\subsubsection{Kernel Functions and Positive Definiteness}

Let $\mathcal{X}$ be an input space, typically $\mathbb{R}^d$, and let $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ be a \textit{kernel function}. A function $k$ is called \textit{positive definite} if for any finite set of points $\{\mathbf{x}_1, \dots, \mathbf{x}_n\} \subset \mathcal{X}$ and any $\mathbf{c} = (c_1, \dots, c_n) \in \mathbb{R}^n$, we have:
\[
\sum_{i=1}^n \sum_{j=1}^n c_i c_j k(\mathbf{x}_i, \mathbf{x}_j) \geq 0.
\]
This property ensures that $k$ can serve as a similarity measure and enables the construction of a unique \ac{rkhs} associated with $k$.

\subsubsection{Constructing RKHS: The Moore-Aronszajn Theorem}

A central result in \ac{rkhs} theory is the \textit{Moore-Aronszajn Theorem}, which guarantees the existence of a unique \ac{rkhs} for any positive definite kernel $k$. The theorem states:

\begin{theorem}[Moore-Aronszajn]
For any positive definite kernel $k$ on $\mathcal{X} \times \mathcal{X}$, there exists a unique Hilbert space $\mathcal{H}$ of functions $f: \mathcal{X} \to \mathbb{R}$ such that:
\begin{enumerate}
    \item $k(\mathbf{x}, \cdot) \in \mathcal{H}$ for all $\mathbf{x} \in \mathcal{X}$,
    \item For every $f \in \mathcal{H}$ and $\mathbf{x} \in \mathcal{X}$, $f(\mathbf{x}) = \langle f, k(\mathbf{x}, \cdot) \rangle_{\mathcal{H}}$, called the \textit{reproducing property}.
\end{enumerate}
\end{theorem}
This theorem allows us to construct $\mathcal{H}$ as the \textit{completion} of the span of the functions $\{k(\mathbf{x}, \cdot) : \mathbf{x} \in \mathcal{X}\}$ with respect to the inner product $\langle \cdot, \cdot \rangle_{\mathcal{H}}$.

\subsubsection{Inner Product and Norm in RKHS}

Given two functions $f, g \in \mathcal{H}$ represented as finite linear combinations of kernel functions, i.e., $f = \sum_{i=1}^n \alpha_i k(\mathbf{x}_i, \cdot)$ and $g = \sum_{j=1}^m \beta_j k(\mathbf{y}_j, \cdot)$, the inner product in $\mathcal{H}$ can be computed as:
\[
\langle f, g \rangle_{\mathcal{H}} = \sum_{i=1}^n \sum_{j=1}^m \alpha_i \beta_j k(\mathbf{x}_i, \mathbf{y}_j).
\]
The corresponding norm is:
\[
\norm{f}_{\mathcal{H}} = \sqrt{\langle f, f \rangle_{\mathcal{H}}} = \sqrt{\sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j k(\mathbf{x}_i, \mathbf{x}_j)}.
\]
This norm represents the ``smoothness'' or ``complexity'' of functions in $\mathcal{H}$, with higher values corresponding to more complex functions.

\subsubsection{Reproducing Property and Point-wise Evaluation}

A crucial property in \ac{rkhs} is the \textit{reproducing property}, which allows pointwise evaluation of functions $f \in \mathcal{H}$ using the inner product. For any $f \in \mathcal{H}$ and $\mathbf{x} \in \mathcal{X}$,
\[
f(\mathbf{x}) = \langle f, k(\mathbf{x}, \cdot) \rangle_{\mathcal{H}}.
\]
This property simplifies function evaluations to inner products, making it possible to evaluate functions at any point without explicitly knowing the form of $f$.

\subsubsection{Representer Theorem}

In many machine learning applications, we wish to minimize a regularized empirical risk functional of the form:
\[
\min_{f \in \mathcal{H}} \sum_{i=1}^n L(y_i, f(\mathbf{x}_i)) + \lambda \norm{f}_{\mathcal{H}}^2,
\]
where $L$ is a loss function (e.g., squared loss for regression) and $\lambda > 0$ is a regularization parameter. The \textit{Representer Theorem} asserts that the solution to this optimization problem can be represented as a linear combination of kernel evaluations at the training points:
\[
f^*(\mathbf{x}) = \sum_{i=1}^n \alpha_i k(\mathbf{x}_i, \mathbf{x}),
\]
where the coefficients $\alpha_i$ can be found by solving a finite-dimensional problem. This reduces the complexity of optimization from the infinite-dimensional space $\mathcal{H}$ to a finite-dimensional space of size $n$, greatly simplifying computation.

\subsubsection{Regularization and Smoothness in RKHS}

The \ac{rkhs} norm $\norm{f}_{\mathcal{H}}$ provides a measure of the smoothness of $f$. When we regularize by minimizing $\norm{f}_{\mathcal{H}}^2$, we control the complexity of the function, effectively penalizing highly oscillatory or rough functions. This form of regularization is particularly relevant in \textit{kernel ridge regression} and \textit{support vector machines}, where the objective function is minimized subject to a smoothness constraint in $\mathcal{H}$.

\subsubsection{Examples of Kernels and Corresponding RKHS}

\begin{itemize}
    \item \textbf{Linear Kernel} $k(\mathbf{x}, \mathbf{y}) = \mathbf{x}^T \mathbf{y}$: The \ac{rkhs} corresponding to this kernel is the space of linear functions $f(\mathbf{x}) = \mathbf{w}^T \mathbf{x}$.
    \item \textbf{Polynomial Kernel} $k(\mathbf{x}, \mathbf{y}) = (\mathbf{x}^T \mathbf{y} + c)^p$: This kernel represents polynomial functions of degree $p$, enabling polynomial feature spaces.
    \item \textbf{Gaussian (RBF) Kernel} $k(\mathbf{x}, \mathbf{y}) = \exp\left(-\frac{\|\mathbf{x} - \mathbf{y}\|^2}{2\sigma^2}\right)$: The \ac{rkhs} for this kernel includes all smooth functions, making it particularly effective for non-linear, smooth function approximation.
\end{itemize}





\section{Introduction to Optimization}
\label{section:optimization_introduction}

Optimization is a fundamental mathematical process focused on identifying the best possible solution from a range of options for a given problem. It is a core technique used in diverse fields, including engineering, economics, machine learning, and operations research. To address this broad spectrum of problems, a consistent way to represent them is necessary. Optimization problems are typically expressed using an objective function \( f \colon \mathbb{R}^d \rightarrow \mathbb{R} \), which transforms an input vector \( \mathbf{x} \) into a scalar output \( y \). The goal is to determine the input \( \mathbf{x}^* \) that results in the optimal output. Although some problems involve minimizing \( f \), this can be equivalently framed as maximizing \( -f \), allowing a uniform representation of all optimization problems as:
\[
\mathbf{x}^* = \arg \min_{\mathbf{x} \in \mathcal{X}} f(\mathbf{x}),
\]
where \( \mathcal{X} \) represents the set of feasible inputs. In certain cases, \( f \) might have a well-defined mathematical form, enabling direct algebraic solution. However, in practical, real-world scenarios, the function is often too complex. In such cases, the function's value must be estimated by evaluating it at various inputs. The key challenge is how to effectively select which inputs to evaluate. Randomly selecting, although straightforward, does not guarantee convergence to an optimal solution within a finite number of evaluations. Therefore, specialized algorithms are required to guide the selection of inputs, ensuring both efficient and effective optimization.


\subsection{Gradient-Based Optimization}
\label{subsection:gradient_based_optimization}

\noindent Gradient-based methods use the gradient (the vector of first-order partial derivatives) of the objective function to iteratively find optimal solutions. The gradient at any point indicates the direction of the steepest ascent of the objective function, so the negative gradient is used for minimization.

The core idea behind gradient-based optimization is to update the decision variable $\mathbf{w}$ iteratively using the following update rule:

\begin{equation}
\mathbf{w}_{k+1} = \mathbf{w}_k + \alpha_k d_k,
\label{eq:general_update}
\end{equation}
where:
\begin{itemize}
    \item $\mathbf{x}_k$ is the decision variable at iteration $k$.
    \item $\mathbf{x}_{k+1}$ is the updated decision variable for the next iteration.
    \item $\alpha_k$ is the learning rate or step size, a positive scalar.
    \item $d_k$ is the direction vector which can be specific for each optimization algorithm.
\end{itemize}

\noindent 
We will now provide a detailed examination of the mathematical foundations of various gradient-based optimization methods, where the objective function is denoted as $f(\mathbf{x})$.

\subsubsection{Gradient Descent}
\label{subsubsection:gradient_descent}

\noindent In its simplest form, \ac{gd} updates the solution by moving in the direction of the negative gradient. The negative gradient points towards the direction of the steepest descent on the error surface. This update pushes the solution in the direction that decreases the objective function the most.

\begin{equation}
d_k = -\nabla f(\mathbf{x}_k)
\end{equation}

\noindent The update rule of Eqn. \ref{eq:general_update} thus becomes:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha_k \nabla f(\mathbf{x}_k),
\label{eq:gd_update}
\end{equation}
where $\nabla f(\mathbf{x}_k) = \left(\frac{\partial f}{\partial \mathbf{x}_1}, \frac{\partial f}{\partial \mathbf{x}_2}, \dots, \frac{\partial f}{\partial \mathbf{x}_d} \right)$ is the gradient vector of $f$ at $x_k$.




\subsubsection{Momentum}
\label{subsubsection:momentum}
Building on the foundational principles of gradient-based optimization, we turn our attention to momentum-based variants of \ac{gd}. These methods aim to improve the convergence rate and stability of the optimization process by incorporating a momentum term.
Momentum introduces a velocity term that accumulates gradients, which helps to smooth the optimization trajectory and escape shallow local minima.

The update rule involves an intermediate variable called velocity $\mathbf{v}_k$.
\begin{equation}
   \mathbf{v}_k = \beta \mathbf{v}_{k-1} - \alpha_k \nabla f(\mathbf{x}_k),
   \label{eq:momentum_v}
\end{equation}
where $\beta$ is the momentum parameter, typically a scalar in the range \( [0, 1) \), controlling the influence of past gradients.Then the update of $\mathbf{x}_k$ becomes:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k + \mathbf{v}_k
\label{eq:momentum_update}
\end{equation}
The momentum term $\beta v_{k-1}$ effectively remembers previous gradients, adding them to the current gradient, which acts as a smoothing filter. This helps the solution continue in a similar direction even if it encounters small hills or valleys on the error surface. Momentum-based methods, such as classical Momentum and \ac{nag}, are widely used for their ability to overcome challenges like slow convergence on flat surfaces and oscillations in steep directions. These methods are particularly beneficial for optimizing non-convex functions, where standard \ac{gd} often struggles.

\subsubsection{Adaptive Learning Rate Algorithms}
\label{subsubsection:adaptive_learning_rates}
In addition to momentum-based methods, adaptive learning rate algorithms have emerged as powerful tools in optimization. These algorithms adjust the learning rate dynamically during the optimization process, allowing for more efficient convergence. By adapting the step size based on the magnitude of gradients or past updates, they aim to overcome challenges such as vanishing or exploding gradients and improve performance across diverse problem settings.

\paragraph{Adagrad}
One of the simplest forms of adaptive learning rate algorithms is Adagrad, which modifies the learning rate for each parameter based on the accumulated squared gradients:

\[
\mathbf{x}_{k+1} = \mathbf{x}_k - \frac{\alpha}{\sqrt{G_k + \epsilon}} \odot \nabla f(\mathbf{x}_k),
\]
where:
\begin{itemize}
    \item \( G_k = \sum_{i=1}^k \nabla f(\mathbf{x}_i) \odot \nabla f(\mathbf{x}_i) \) is the element-wise sum of squared gradients up to iteration \( k \),
    \item \( \epsilon \) is a small positive constant to avoid division by zero,
    \item \( \odot \) denotes element-wise multiplication,
    \item \( \alpha \) is the initial learning rate.
\end{itemize}

\paragraph{Adam}
\label{paragraph:adam}
Another popular algorithm, \ac{adam} (Adaptive Moment Estimation), introduced by \citet{kingma2014adam}, builds upon Adagrad to address its limitations, particularly its tendency to excessively decay the learning rate in the presence of accumulated gradients. \ac{adam} combines momentum with adaptive learning rates and maintains both a first-moment estimate (momentum) and a second-moment estimate (variance) for more robust optimization. This dual approach helps stabilize parameter updates and adaptively scales learning rates for each parameter:

\begin{align}
\mathbf{m}_k &= \beta_1 \mathbf{m}_{k-1} + (1 - \beta_1) \nabla f(\mathbf{x}_k), \\  
\mathbf{v}_k &= \beta_2 \mathbf{v}_{k-1} + (1 - \beta_2) (\nabla f(\mathbf{x}_k))^2,
\end{align}

\noindent where $\mathbf{m}_k$ is the estimate for the first-order moment (mean of the gradients), and $\mathbf{v}_k$ is the estimate for the second-order moment (uncentered variance of the gradients). The hyperparameters $\beta_1 \in [0, 1)$ and $\beta_2 \in [0, 1)$ control the exponential decay rates of the moving averages for the first and second moments, respectively. Common default values are $\beta_1 = 0.9$ and $\beta_2 = 0.999$, as recommended in the original paper. 

To address bias introduced by initializing $\mathbf{m}_k$ and $\mathbf{v}_k$ to zero at the beginning of training, corrected estimates are computed as:

\begin{align}
    \hat{\mathbf{m}}_k &= \frac{\mathbf{m}_k}{1-\beta_1^k}, \\  
    \hat{\mathbf{v}}_k &= \frac{\mathbf{v}_k}{1-\beta_2^k},
\end{align}

\noindent These bias-corrected estimates ensure that $\hat{\mathbf{m}}_k$ and $\hat{\mathbf{v}}_k$ are unbiased, particularly during the initial steps when $k$ is small. The final parameter update step is given by:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha \frac{\hat{\mathbf{m}}_k}{\sqrt{\hat{\mathbf{v}}_k} + \epsilon},
\end{equation}

\noindent where $\alpha$ is the learning rate, typically set to a small value (e.g., $0.001$ in many applications), and $\epsilon$ is a small constant added to prevent division by zero. A typical value for $\epsilon$ is $10^{-8}$. The inclusion of $\epsilon$ improves numerical stability, particularly in cases where $\hat{\mathbf{v}}_k$ approaches zero.

\paragraph{RMSProp}
\label{paragraph:rmsprop}
\ac{rmsprop}, proposed by \citet{tieleman2012lecture}, is another popular optimization algorithm designed to overcome the drawbacks of Adagrad in non-convex optimization problems. While Adagrad's aggressive learning rate decay can hinder optimization over time, \ac{rmsprop} mitigates this by using an exponentially decaying average of squared gradients to adapt learning rates. This mechanism ensures a more stable and efficient convergence, particularly in neural network training. The update rule for \ac{rmsprop} is given by:

\begin{align}
\mathbf{v}_k &= \beta \mathbf{v}_{k-1} + (1 - \beta) (\nabla f(\mathbf{x}_k))^2, \\
\mathbf{x}_{k+1} &= \mathbf{x}_k - \alpha \frac{\nabla f(\mathbf{x}_k)}{\sqrt{\mathbf{v}_k} + \epsilon},
\end{align}

\noindent where $\mathbf{v}_k$ represents the moving average of the squared gradients, and $\beta \in [0, 1)$ is a hyperparameter controlling the decay rate of this average, typically set to $0.9$. The term $\epsilon$ is a small positive constant, commonly $10^{-8}$, added for numerical stability to avoid division by zero. The adaptive scaling of gradients via $\sqrt{\mathbf{v}_k}$ allows \ac{rmsprop} to handle large gradients effectively while maintaining a consistent learning rate for smaller gradients. This balance makes \ac{rmsprop} particularly well-suited for deep learning applications involving non-stationary loss functions.


\subsubsection{Conjugate Gradient}
\label{subsubsection:conjugate_gradient}

The \ac{cg} method is a powerful optimization algorithm primarily used for solving large-scale, unconstrained quadratic minimization problems, particularly when the Hessian matrix is too large to store explicitly. Introduced by \citet{hestenes1952methods}, \ac{cg} iteratively updates the solution by combining gradient descent with a conjugacy condition, ensuring that successive search directions are mutually conjugate with respect to the quadratic form. This approach enables faster convergence compared to standard gradient descent. At each iteration, the solution is updated as:

\begin{align}
\mathbf{x}_{k+1} &= \mathbf{x}_k + \alpha_k \mathbf{p}_k, \\
\mathbf{p}_{k+1} &= -\nabla f(\mathbf{x}_{k+1}) + \beta_k \mathbf{p}_k,
\end{align}
where $\mathbf{p}_k$ is the search direction, $\alpha_k$ is the step size determined by line search, and $\beta_k$ is the conjugate gradient coefficient. The value of $\beta_k$ can be computed using various formulas, including Fletcher-Reeves and Polak-Ribiere. The Fletcher-Reeves formula defines $\beta_k$ as:

\begin{equation}
\beta_k^{\text{FR}} = \frac{\|\nabla f(\mathbf{x}_{k+1})\|^2}{\|\nabla f(\mathbf{x}_k)\|^2},
\end{equation}

\noindent while the Polak-Ribiere formula introduces a more flexible approach:

\begin{equation}
\beta_k^{\text{PR}} = \frac{\nabla f(\mathbf{x}_{k+1})^\top (\nabla f(\mathbf{x}_{k+1}) - \nabla f(\mathbf{x}_k)}{\|\nabla f(\mathbf{x}_k)\|^2}.
\end{equation}

\noindent The Polak-Ribiere method can handle non-quadratic objective functions more effectively and may reset $\beta_k$ to zero when its value becomes negative, reverting to steepest descent for better stability. These formulations ensure that successive directions remain conjugate, allowing \ac{cg} to converge in at most $n$ iterations for a quadratic objective with $n$ variables. The \ac{cg} method is particularly efficient for sparse systems and avoids the need to compute or store the full Hessian matrix, making it well-suited for high-dimensional problems. Extensions of \ac{cg}, such as the nonlinear Conjugate Gradient method, adapt these principles to more general optimization scenarios.

\subsubsection{Newton's Method}
\label{subsubsection:newtons_method}
Newton's Method is a second-order optimization algorithm widely used for solving unconstrained optimization problems. It leverages both gradient and curvature information to achieve faster convergence, especially near the optimum. At each iteration, Newton's Method approximates the objective function $f(\mathbf{x})$ as a second-order Taylor expansion around the current point $\mathbf{x}_k$ and updates the parameters by solving the resulting quadratic model. The update rule is given by:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k - \mathbf{H}_k^{-1} \nabla f(\mathbf{x}_k),
\end{equation}

\noindent where $\nabla f(\mathbf{x}_k)$ is the gradient vector, and $\mathbf{H}_k$ is the Hessian matrix, representing the second-order partial derivatives of $f(\mathbf{x})$ with respect to the parameters. The inverse Hessian $\mathbf{H}_k^{-1}$ scales and modifies the gradient direction to account for the curvature of the objective function, allowing for more precise steps toward the optimum.

One of the key advantages of Newton's Method is its quadratic convergence rate when the initial guess is sufficiently close to the solution. However, the method also has notable challenges. Computing and inverting the Hessian matrix can be computationally expensive for high-dimensional problems, making it less practical for large-scale optimization. To address this, approximations like the quasi-Newton methods (e.g., BFGS) are often used to estimate the inverse Hessian iteratively without explicitly forming it.

Newton's Method is particularly effective for convex optimization problems where the Hessian is positive definite. In non-convex settings, the method may encounter difficulties such as saddle points or negative curvature, which can lead to divergence. To mitigate this, techniques such as adding a regularization term to the Hessian (e.g., trust-region methods) or using a modified update rule are commonly employed.



% \noindent Newton's method uses the second-order derivative information (the Hessian matrix, denoted by $\mathbf{H}f(x)$) to guide the optimization. The update rule is given by:

% \begin{equation}
% x_{k+1} = x_k - \left( \mathbf{H}f(x_k) \right)^{-1} \nabla f(x_k)
% \end{equation}

% \noindent where $\left( \mathbf{H}f(x_k) \right)^{-1}$ is the inverse of the Hessian matrix evaluated at $x_k$.

% \textit{Intuition:} Newton's method finds the minimum of the quadratic approximation of the function at $x_k$. By using the curvature information given by the Hessian, it can take larger steps toward the minimum compared to gradient descent.

\subsubsection{Quasi-Newton Methods (BFGS, L-BFGS)}
\label{subsubsection:quasi_newton}
Quasi-Newton methods are iterative optimization algorithms that approximate Newton's Method while avoiding the explicit computation and inversion of the Hessian matrix, making them more computationally efficient for high-dimensional problems. Among these, the Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is one of the most widely used. At each iteration, the search direction $\mathbf{p}_k$ is computed as:

\begin{equation}
\mathbf{p}_k = -\mathbf{H}_k^{-1} \nabla f(\mathbf{x}_k),
\end{equation}

\noindent where $\mathbf{H}_k^{-1}$ is the approximation of the inverse Hessian matrix. The parameter update is then performed as:

\begin{equation}
\mathbf{x}_{k+1} = \mathbf{x}_k + \alpha_k \mathbf{p}_k,
\end{equation}

\noindent with $\alpha_k$ representing the step size, typically determined through a line search. In the BFGS algorithm, the approximation of the Hessian matrix $\mathbf{H}_k$ itself is updated iteratively using the formula:

\begin{equation}
\mathbf{H}_{k+1} = \mathbf{H}_k 
+ \frac{\mathbf{y}_k \mathbf{y}_k^\top}{\mathbf{y}_k^\top \mathbf{s}_k} 
- \frac{\mathbf{H}_k \mathbf{s}_k \mathbf{s}_k^\top \mathbf{H}_k}{\mathbf{s}_k^\top \mathbf{H}_k \mathbf{s}_k},
\end{equation}

\noindent where $\mathbf{s}_k = \mathbf{x}_{k+1} - \mathbf{x}_k$ and $\mathbf{y}_k = \nabla f(\mathbf{x}_{k+1}) - \nabla f(\mathbf{x}_k)$. The first term ensures the positive definiteness of $\mathbf{H}_{k+1}$, while the second term adjusts the curvature information. To maintain computational feasibility, $\mathbf{H}_k$ is often initialized as a scaled identity matrix, $\mathbf{H}_0 = \gamma \mathbf{I}$, where $\gamma$ is a positive scalar.

For large-scale problems where storing or computing $\mathbf{H}_k$ or $\mathbf{H}_k^{-1}$ explicitly is impractical, the Limited-memory BFGS (L-BFGS) algorithm is employed. Instead of maintaining the full Hessian or its inverse, L-BFGS uses a limited number of recent $\mathbf{s}_k$ and $\mathbf{y}_k$ vectors to implicitly compute the search direction $\mathbf{p}_k$. This reduces memory and computational overhead, making L-BFGS well-suited for problems with millions of variables.

Quasi-Newton methods such as BFGS and L-BFGS offer superlinear convergence rates and efficiently balance the rapid convergence of Newton's Method with the simplicity of gradient descent. Their robustness and efficiency make them standard tools for large-scale optimization tasks, particularly in machine learning and numerical optimization.



% \noindent The BFGS update can be summarized as:

% \begin{equation}
%     x_{k+1} = x_k - \alpha_k B_k^{-1} \nabla f(x_k)
% \end{equation}

% \noindent where $B_k$ is an approximation of the inverse Hessian matrix and $\alpha_k$ is computed by line search. $B_{k+1}$ can be computed using previous estimates using:

% \begin{equation}
%     B_{k+1} = B_k + \frac{y_ky_k^T}{y_k^Ts_k} - \frac{(B_ks_k)(B_ks_k)^T}{s_k^T B_k s_k}
% \end{equation}

% \noindent where $s_k = x_{k+1} - x_k$ and $y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$

% \textit{Intuition:} These methods use approximations of the inverse Hessian, which allow it to take larger steps toward the minimum compared to gradient descent but are also more computationally efficient.

\subsection{Derivative-Free Optimization}
\label{subsection:derivative_free_optimization}
The optimization techniques discussed so far, including Gradient Descent, Newton's Method, and their variants, rely fundamentally on gradient information to navigate the search space and converge to an optimum. These methods are well-suited for problems where derivatives are readily available, accurate, and computationally inexpensive. However, many real-world optimization problems present challenges such as noisy evaluations, non-differentiable objective functions, or black-box functions where derivatives are unavailable or impractical to compute.  

In such scenarios, \ac{dfo}  methods provide a viable alternative. Rather than relying on gradient information, \ac{dfo} algorithms use only function evaluations to iteratively explore the search space and identify optima. These methods are particularly effective in settings where the objective function is expensive to evaluate, non-convex, or highly constrained. The following sections introduce various \ac{dfo} techniques, focusing on their underlying principles, advantages, and applicability to modern optimization challenges.

\subsubsection{Nelder-Mead Method}
\label{subsubsection:nelder_mead}
One of the most well-known Derivative-Free Optimization methods is the Nelder-Mead method, introduced by \citet{nelder1965simplex}. This simplex-based algorithm is designed for unconstrained optimization of nonlinear functions and operates by iteratively refining a geometric simplex, which is a set of \(n+1\) points in an \(n\)-dimensional space.

At each iteration, the algorithm evaluates the objective function at the vertices of the simplex and replaces the worst-performing vertex with a better candidate point. The algorithm employs four main operations to modify the simplex: reflection, expansion, contraction, and shrinkage. These operations are defined as follows:

\begin{itemize}
    \item \textbf{Reflection}: Reflect the worst vertex \(\mathbf{x}_h\) through the centroid \(\mathbf{c}\) of the remaining vertices:
    \begin{equation}
    \mathbf{x}_r = \mathbf{c} + \alpha (\mathbf{c} - \mathbf{x}_h),
    \end{equation}
    where \(\alpha > 0\) is the reflection coefficient, typically set to 1.

\item \textbf{Expansion}: If the reflected point \(\mathbf{x}_r\) is better than all other vertices, an expansion is attempted:
\begin{equation}
\mathbf{x}_e = \mathbf{c} + \gamma (\mathbf{x}_r - \mathbf{c}),
\end{equation}
where \(\gamma > 1\) is the expansion coefficient.

\item \textbf{Contraction}: If the reflected point \(\mathbf{x}_r\) is not better than the current best, a contraction is performed:
\begin{equation}
\mathbf{x}_c = \mathbf{c} + \rho (\mathbf{x}_h - \mathbf{c}),
\end{equation}
where \(0 < \rho < 1\) is the contraction coefficient.

\item \textbf{Shrinkage}: If no improvement is observed, the entire simplex is shrunk towards the best vertex:
\begin{equation}
\mathbf{x}_i = \mathbf{x}_b + \sigma (\mathbf{x}_i - \mathbf{x}_b),
\end{equation}
for all vertices \(i\), where \(0 < \sigma < 1\) is the shrinkage coefficient, and \(\mathbf{x}_b\) is the best vertex.
\end{itemize}


The Nelder-Mead method does not require gradient information and is particularly suited for problems where the objective function is noisy, discontinuous, or non-differentiable. However, the algorithm may converge to non-stationary points in certain cases, especially in high-dimensional spaces or poorly scaled functions. Despite these limitations, its simplicity and effectiveness have made it a widely used tool in derivative-free optimization.

\subsubsection{Genetic Algorithm (GA)}
\label{subsubsection:genetic_algorithm}

Another prominent approach in \ac{dfo} is the \ac{ga}, inspired by the principles of natural selection and evolutionary biology. \acp{ga} are population-based meta-heuristic methods that iteratively evolve a population of candidate solutions to optimize an objective function. These algorithms are particularly effective for global optimization in complex, multimodal, or discrete search spaces.

A typical \ac{ga} involves the following key steps:

\begin{itemize}
    \item \textbf{Initialization}: A population of candidate solutions, often represented as binary strings (or other encodings), is randomly generated.
    
    \item \textbf{Evaluation}: The objective function is evaluated for each candidate solution, assigning a fitness value that measures its quality.
    
    \item \textbf{Selection}: Candidate solutions are selected for reproduction based on their fitness. Common selection strategies include roulette wheel selection, tournament selection, and rank-based selection.
    
    \item \textbf{Crossover (Recombination)}: Pairs of selected candidates undergo crossover to produce offspring by exchanging segments of their representations. For example, in single-point crossover, a crossover point is chosen, and the segments are swapped:
    \begin{equation}
    \text{Offspring 1} = \text{Parent 1}_{[1:c]} + \text{Parent 2}_{[c+1:end]},
    \end{equation}
    \begin{equation}
    \text{Offspring 2} = \text{Parent 2}_{[1:c]} + \text{Parent 1}_{[c+1:end]},
    \end{equation}
    where \(c\) is the crossover point.
    
    \item \textbf{Mutation}: To maintain diversity in the population and avoid premature convergence, mutation is applied to the offspring by randomly altering one or more bits or values in their representation:
    \begin{equation}
    \text{Mutated Offspring} = \text{Original Offspring} + \delta,
    \end{equation}
    where \(\delta\) represents a small random change.
    
    \item \textbf{Replacement}: The offspring replace some or all members of the current population, creating a new generation.
\end{itemize}

This process repeats over multiple generations, gradually improving the quality of solutions. The stopping criterion can be a fixed number of generations, a threshold for fitness improvement, or other problem-specific conditions.
\acp{ga} are highly flexible and can be adapted to a wide range of optimization problems by tailoring the encoding scheme, fitness function, and genetic operators. While \acp{ga} do not guarantee convergence to a global optimum, their ability to explore large and complex search spaces makes them a popular choice for black-box and combinatorial optimization problems.

\subsubsection{Simulated Annealing (SA)}
\label{subsubsection:simulated_annealing}
Another widely used Derivative-Free Optimization method is \ac{sa}, inspired by the annealing process in metallurgy, where a material is heated and then slowly cooled to reduce defects and reach a more stable state. First proposed by \citet{kirkpatrick1983optimization}, SA is a probabilistic technique for finding a global optimum in a large search space, particularly for non-convex or combinatorial optimization problems.

\ac{sa} operates by iteratively exploring the solution space, accepting new solutions based on their quality and a probabilistic acceptance criterion. The main steps are as follows:

\begin{itemize}
    \item \textbf{Initialization}: Start with an initial solution \(\mathbf{x}_0\) and an initial temperature \(T_0\). Define a cooling schedule to gradually reduce the temperature.
    
    \item \textbf{Generate a Neighbor}: At each iteration \(k\), generate a new candidate solution \(\mathbf{x}_{k+1}\) by applying a small random perturbation to the current solution \(\mathbf{x}_k\).
    
    \item \textbf{Acceptance Criterion}: Evaluate the change in objective function value, \(\Delta f = f(\mathbf{x}_{k+1}) - f(\mathbf{x}_k)\). Accept the new solution with a probability:
    \begin{equation}
    P = 
    \begin{cases} 
    1 & \text{if } \Delta f < 0, \\
    \exp\left(-\frac{\Delta f}{T_k}\right) & \text{if } \Delta f \geq 0,
    \end{cases}
    \end{equation}
    where \(T_k\) is the temperature at iteration \(k\). This allows the algorithm to escape local minima by occasionally accepting worse solutions.
    
    \item \textbf{Update the Temperature}: Gradually decrease the temperature according to a cooling schedule, such as \(T_{k+1} = \alpha T_k\), where \(0 < \alpha < 1\) is the cooling rate.
\end{itemize}

The algorithm terminates when the temperature reaches a predefined threshold or after a fixed number of iterations. By balancing exploration and exploitation through the temperature parameter, Simulated Annealing is capable of escaping local optima and exploring the global search space effectively.

Despite its simplicity, the performance of Simulated Annealing depends significantly on the cooling schedule and parameter settings. Its versatility and ability to handle complex optimization problems have made it a popular choice in fields ranging from logistics to engineering design.


\subsubsection{Covariance Matrix Adaptation Evolution Strategy (CMA-ES)}
\label{subsubsection:cma_es}

One of the most advanced and widely used Derivative-Free Optimization methods is the \ac{cmaes}, introduced by \citet{hansen2001completely}. \ac{cmaes} is a population-based algorithm that excels in solving complex, multimodal, and high-dimensional optimization problems. It is particularly effective for continuous domains and has been successfully applied in various scientific and engineering applications.
\ac{cmaes} operates by iteratively evolving a population of candidate solutions, using a multivariate normal distribution to sample new solutions and adaptively updating its parameters. The main components are as follows:

\begin{itemize}
    \item \textbf{Initialization}: Start with an initial mean vector \(\mathbf{m}_0\), an initial covariance matrix \(\mathbf{C}_0 = \sigma_0^2 \mathbf{I}\), and a step size \(\sigma_0\). Define the population size \(\lambda\) and the weights $w$ for weighted recombination.
    
    \item \textbf{Sampling}: At each iteration \(k\), generate \(\lambda\) offspring solutions by sampling from the current multivariate normal distribution:
    \begin{equation}
    \mathbf{x}_i^{(k)} \sim \mathcal{N}(\mathbf{m}_k, \sigma_k^2 \mathbf{C}_k), \quad i = 1, \dots, \lambda.
    \end{equation}
    
    \item \textbf{Evaluation and Selection}: Evaluate the objective function for each offspring \(\mathbf{x}_i^{(k)}\) and rank them based on their fitness. Select the top \(\mu\) solutions (\(\mu < \lambda\)) for recombination.
    
    \item \textbf{Recombination}: Compute the new mean vector as a weighted sum of the selected solutions:
    \begin{equation}
    \mathbf{m}_{k+1} = \sum_{i=1}^\mu w_i \mathbf{x}_{i:\lambda}^{(k)},
    \end{equation}
    where \(\mathbf{x}_{i:\lambda}\) denotes the \(i\)-th best solution.
    
    \item \textbf{Covariance Matrix Adaptation}: The covariance matrix \(\mathbf{C}_k\) is updated to reflect the distribution of successful steps, capturing correlations among variables:
    \begin{equation}
    \mathbf{C}_{k+1} = (1 + c_1 \delta(h_\sigma) - c_1 - c_\mu \sum_{j=1}^\mu w_j) \mathbf{C}_k + c_1 \mathbf{p}_c \mathbf{p}_c^\top + c_\mu \sum_{i=1}^\lambda w_i^\text{o} \mathbf{y}_{i:\lambda} \mathbf{y}_{i:\lambda}^\top,
    \end{equation}
    where \(\mathbf{p}_c\) is the evolution path for the covariance matrix, \(\mathbf{y}_{i:\lambda} = \frac{\mathbf{x}_{i:\lambda} - \mathbf{m}_k}{\sigma_k}\), and \(c_1, c_\mu\) are adaptation parameters. The term \(\delta(h_\sigma)\) incorporates the Heaviside function:
    \begin{equation}
    h_\sigma = 
    \begin{cases}
    1, & \text{if } \frac{\|\mathbf{p}_\sigma\|}{\sqrt{1 - (1 - c_\sigma)^{2(g+1)}}} < \left(1.4 + \frac{2}{n+1}\right) \mathbb{E}\|\mathcal{N}(\mathbf{0}, \mathbf{I})\|, \\
    0, & \text{otherwise}.
    \end{cases}
    \end{equation}
    This condition prevents a too fast increase of axes of \(\mathbf{C}_k\) when the step size is initially set too small or the objective function changes over time. The weights \(w_i^\text{o}\) are calculated as:  
    \begin{equation}
    w_i^\text{o} = w_i \times 
    \begin{cases} 
    1, & \text{if } w_i \geq 0, \\ 
    \frac{n}{\|\mathbf{C}^{-\frac{1}{2}} \mathbf{y}_{i:\lambda}\|^2}, & \text{otherwise}.
    \end{cases}
    \end{equation}
    Here, \(n\) is the problem dimension, and \(\mathbf{C}^{-\frac{1}{2}}\) is the inverse square root of the covariance matrix. This adjustment ensures that the covariance matrix adaptation is robust and efficient, even in challenging optimization landscapes.

    
    \item \textbf{Evolution Paths}: CMA-ES uses evolution paths to track the progress of the search and guide the adaptation of the covariance matrix and step size:
    \begin{itemize}
        \item The evolution path for the covariance matrix, \(\mathbf{p}_c\), accumulates successive step directions:
        \begin{equation}
        \mathbf{p}_c = (1 - c_c) \mathbf{p}_c + \sqrt{c_c (2 - c_c) \mu_\text{eff}} \frac{\mathbf{m}_{k+1} - \mathbf{m}_k}{\sigma_k},
        \end{equation}
        where \(c_c\) is the learning rate, and \(\mu_\text{eff}\) is the effective number of selected solutions.
        
        \item The evolution path for step size, \(\mathbf{p}_\sigma\), adapts the step size dynamically based on the normalized step directions:
        \begin{equation}
        \mathbf{p}_\sigma = (1 - c_\sigma) \mathbf{p}_\sigma + \sqrt{c_\sigma (2 - c_\sigma) \mu_\text{eff}} \mathbf{C}_k^{-\frac{1}{2}} \frac{\mathbf{m}_{k+1} - \mathbf{m}_k}{\sigma_k},
        \end{equation}
        where \(c_\sigma\) is the learning rate for step size adaptation.
    \end{itemize}
    
    \item \textbf{Step Size Adaptation}: Adjust the step size \(\sigma_k\) using the evolution path \(\mathbf{p}_\sigma\):
    \begin{equation}
    \sigma_{k+1} = \sigma_k \exp\left(\frac{c_\sigma}{d_\sigma} \left(\frac{\|\mathbf{p}_\sigma\|}{\mathbb{E}[\|\mathcal{N}(0, \mathbf{I})\|]} - 1\right)\right),
    \end{equation}
    where \(d_\sigma\) is a damping factor.
\end{itemize}

\ac{cmaes} balances exploration and exploitation through the adaptive covariance matrix and step size, making it highly robust and effective in diverse optimization tasks. While computationally intensive due to the covariance matrix updates, its ability to navigate complex landscapes and adapt to problem structure has made \ac{cmaes} a benchmark algorithm in the field of black-box optimization.


\section{Black-box Optimization}
\label{section:black_box_optimization}
\subsection{Introduction to Black-box Optimization}
In previous sections, Gradient-Based Optimization and Derivative-Free Optimization are introduced. While gradient-based methods leverage explicit derivative information and derivative-free methods only rely on function evaluations, both assume some level of access to the structure or behavior of the objective function. \acf{bo}, in contrast, deals with scenarios where the objective function is a black-box, allowing evaluation only through expensive and potentially noisy queries. This requires suitable approaches that balance efficiency and accuracy while navigating the unknown landscape of the function.  Mathematically, \ac{bo} is the process of finding the optimal solution to a problem where the objective function  $f(\mathbf{x})$ is unknown or analytically intractable. The function $f(\mathbf{x})$ is accessible only through point-wise evaluations, typically via simulations, experiments, or complex computations. This makes \ac{bo} particularly challenging, as each evaluation can be computationally expensive or time-consuming.  

\paragraph{Problem Definition}

The goal of black-box optimization is to solve the following problem:  
\[
x^* = \arg\min_{x \in \mathcal{X}} f(x),
\]  
where $\mathcal{X} \subseteq \mathbb{R}^d$ represents the feasible domain, which may include constraints. Unlike traditional optimization methods, \ac{bo} assumes no explicit knowledge of \( f(x) \), such as its gradients or function form.  Black-box functions often exhibit the following characteristics:  
\begin{itemize}
    \item \textbf{Expensive Evaluations}: Each query to \( f(x) \) incurs a high computational or physical cost. 
    \item \textbf{High Dimensionality}: The input space \( \mathcal{X} \) may be high-dimensional, increasing the difficulty of efficient exploration. 
    \item \textbf{Noisy Observations}: Evaluations of \( f(x) \) may be corrupted by noise, represented as:  
   \[
   y(x) = f(x) + \epsilon,
   \]  
   where $\epsilon$ is the noise and often assumed to follow a Gaussian distribution $\epsilon \sim \mathcal{N}(0, \sigma^2)$
\item \textbf{Complex Constraints}: The search space \( \mathcal{X} \) may include unknown or black-box constraints \( g_i(x) \leq 0 \), \( i = 1, \dots, m \)
\end{itemize}

% ### Exploration vs. Exploitation  

A key challenge in \ac{bo} is balancing ``exploration'', searching unvisited areas to discover new optima, and ``exploitation'', refining evaluations near promising regions. This balance is often managed using acquisition functions \( a(x) \), guiding the search by estimating the utility of sampling at \( x \).  

% ### Mathematical Framework  

Let \( \mathcal{D}_n = \{(x_i, y_i)\}_{i=1}^n \) represent the data collected after \( n \) evaluations, where \( x_i \in \mathcal{X} \) are the sampled points and \( y_i = f(x_i) + \epsilon \) are the observed responses. A surrogate model \( \hat{f}(x | \mathcal{D}_n) \) is constructed to approximate \( f(x) \) based on \( \mathcal{D}_n \). The next evaluation point \( x_{n+1} \) is chosen to optimize the acquisition function:  
\[
x_{n+1} = \arg\min_{x \in \mathcal{X}} a(x | \mathcal{D}_n).
\]  



% ### Applications of Black-box Optimization  

% Black-box optimization is widely used in various domains, including:  
% 1. **Hyperparameter Optimization:** Selecting optimal parameters for machine learning models.  
% 2. **Engineering Design:** Optimizing structures, materials, or systems with simulation-based evaluations.  
% 3. **Scientific Discovery:** Identifying optimal conditions for experiments or chemical processes.  

% ### Challenges and Future Directions  

% Black-box optimization faces several challenges, including scalability to high-dimensional problems, handling multi-objective or constrained optimization, and improving efficiency for noisy or dynamic environments. Recent advances, such as surrogate models, physics-informed optimization, and deep learning-based methods, aim to address these limitations and extend the applicability of BBO to more complex scenarios.  

\subsection{Surrogates Models}
In black-box optimization, the objective function is often expensive to evaluate, high-dimensional, and analytically intractable. To address these challenges, surrogate models are employed as efficient approximations of the true objective function. These models, such as \acfp{gp}, \acfp{dnn}, or \acfp{rf}, act as computationally inexpensive proxies that capture the underlying structure of the objective function using a limited number of evaluations. By iteratively refining the surrogate model with newly acquired data, the optimization process can efficiently explore and exploit the search space. Surrogate models not only reduce the number of expensive function evaluations required but also provide probabilistic estimates of uncertainty, enabling the use of acquisition functions to guide the search toward promising regions. This approach is foundational in frameworks such as Bayesian optimization, where the balance between exploration and exploitation is critical.



\subsubsection{Deep Neural Network and Neural Tangent Kernel}
\label{background:ntk}

% \subsection{Physics-Informed Neural Networks}
\subsection{Utility-Based Acquisition Functions}
\label{section:acquisition_functions}

In Bayesian optimization, the objective is to efficiently locate the global optimum of an expensive-to-evaluate function, a task particularly challenging in scenarios where gradient information is unavailable or unreliable \citep{mockus1978application,jones1998efficient}. Traditional optimization methods, often reliant on such gradient information, fall short in these contexts, necessitating alternative strategies. Bayesian optimization addresses this problem by employing a probabilistic surrogate model, often a Gaussian Process (GP) \citep{rasmussen2006gaussian}, to approximate the objective function. This surrogate model provides not only predictions of the objective function's values at unobserved points but also measures of uncertainty associated with these predictions. The selection of the next evaluation point is critical to the success of this methodology and is guided by \textit{acquisition functions}. These functions quantify the utility or desirability of evaluating a given point based on the model’s predictions and uncertainty, effectively navigating the trade-off between \textit{exploration} of less-sampled regions and \textit{exploitation} of regions that have yielded promising results [Citation1].  The choice of acquisition function plays a pivotal role in the optimization process, determining the efficiency and effectiveness of convergence to the global optimum [Citation2, Citation3]. The following subsections will detail several widely used utility-based acquisition functions, emphasizing their mathematical formulations, the underlying intuitions, and their relative strengths and weaknesses.
\subsubsection{Probability of Improvement}
\label{section:pi}

The \ac{PI} acquisition function, introduced by \citet{kushner1964new}, is a straightforward yet effective strategy for guiding Bayesian optimization, with the specific purpose of improving upon the current best-observed value. Unlike \ac{ucb} and \ac{ts}, \ac{PI} does not explicitly incorporate the uncertainty of the prediction directly into its selection criteria; instead, it focuses only on the probability of improvement over the current best observation. The simplicity and computational efficiency of \ac{PI} have made it a popular technique for many \ac{bo} problems, especially when computational resources are limited, or quick convergence is preferred.

Given the best-observed objective function value \(f(\mathbf{x}^+)\) up to the current iteration, and the predictive mean \(\mu(\mathbf{x})\) and standard deviation \(\sigma(\mathbf{x})\) of a point \(\mathbf{x}\), the \ac{PI} function is defined as:
\begin{equation}
\alpha_{\text{PI}}(\mathbf{x}) = P(f(\mathbf{x}) > f(\mathbf{x}^+) + \xi) = \Phi\left( \frac{\mu(\mathbf{x}) - f(\mathbf{x}^+) - \xi}{\sigma(\mathbf{x})} \right)
\label{eq:pi_formula}
\end{equation}
where:
\begin{itemize}
    \item \(f(\mathbf{x}^+)\) is the best function value observed so far in the optimization process;
    \item \(\mu(\mathbf{x})\) is the predicted mean of the objective function at the point \(\mathbf{x}\);
    \item \(\sigma(\mathbf{x})\) is the predicted standard deviation (or uncertainty) of the objective function at the point \(\mathbf{x}\);
    \item \(\Phi(\cdot)\) is the \ac{cdf} of the standard Normal distribution, which is used since it is assumed that the objective function \(f(\mathbf{x})\) is distributed as a Gaussian;
    \item \(\xi \geq 0\) is an optional hyperparameter that allows a trade-off between exploration and exploitation.
\end{itemize}

Equation \eqref{eq:pi_formula} mathematically expresses the probability that the true objective function value at a location \(\mathbf{x}\), denoted as \(f(\mathbf{x})\), will be greater than the current best value seen so far, \(f(\mathbf{x}^+)\), with an optional margin \(\xi\) to encourage exploration. This probability is computed under the assumption that \(f(\mathbf{x})\) follows a Gaussian distribution with the predicted mean \(\mu(\mathbf{x})\) and standard deviation \(\sigma(\mathbf{x})\), as modeled by the surrogate function (e.g., \ac{gp}). The term \(\frac{\mu(\mathbf{x}) - f(\mathbf{x}^+) - \xi}{\sigma(\mathbf{x})}\) represents the normalized distance from the predicted mean to the current best value, adjusted by the exploration parameter \(\xi\) and the uncertainty \(\sigma(\mathbf{x})\).

The \ac{PI} function prioritizes points that are likely to lead to an improvement over the current best-observed value. This intuition makes it useful for situations when the optimization goal is primarily about exploiting promising regions of the search space. However, this behavior contrasts with acquisition functions such as Expected Improvement (\ac{ei}), which incorporate both the magnitude and probability of improvement, or Upper Confidence Bound (\ac{ucb}), which balances exploration and exploitation more explicitly.

While \ac{PI} is straightforward to understand and implement due to its simple formulation, it has limitations. A key benefit of PI is its computational efficiency, requiring only basic calculations to determine the next evaluation point. Additionally, it typically performs well in scenarios where the objective function is unimodal or when exploitation is prioritized. However, \ac{PI}'s greedy nature can result in overly local search behavior. By focusing exclusively on high-probability regions, it may neglect areas with higher uncertainty, leading to poor exploration. This can cause \ac{PI} to converge prematurely to local optima, particularly in multi-modal or high-dimensional optimization problems where global exploration is crucial.

To mitigate this issue, the exploration parameter \(\xi\) can be introduced, increasing the focus on regions with higher uncertainty. Larger values of \(\xi\) encourage exploration, but tuning this parameter is non-trivial and problem-specific. 

\subsubsection{Expected Improvement}
\label{section:ei}

\acf{ei} is an acquisition function that extends the concept of \ac{PI} by quantifying the expected magnitude of improvement, rather than simply the probability of improvement \citep{mockus1978application, jones1998efficient}. 

The \ac{ei} is defined as the expected value of the maximum between zero and the difference of the current point and the best-observed objective function value. Formally:
\begin{equation}
\alpha_{\text{EI}}(\mathbf{x}) = \mathbb{E} [\max(0, f(\mathbf{x}^+) - f(\mathbf{x}))].
\label{eq:ei_definition}
\end{equation}
Assuming that the values of the objective function \(f(\mathbf{x})\) at a location \(\mathbf{x}\) are normally distributed with mean \(\mu(\mathbf{x})\) and standard deviation \(\sigma(\mathbf{x})\), the expected improvement in equation \eqref{eq:ei_definition} can be rewritten as:
\begin{equation}
    \alpha_{\text{EI}}(\mathbf{x}) = \sigma(\mathbf{x}) [ z \Phi(z) + \phi(z) ],
    \label{eq:ei_formula}
\end{equation}
where: \(z =  \frac{f(\mathbf{x}^+) - \mu(\mathbf{x})}{\sigma(\mathbf{x})}\); \(f(\mathbf{x}^+)\) is the best function value observed so far in the optimization process; \(\mu(\mathbf{x})\) is the predicted mean of the objective function at point \(\mathbf{x}\); \(\sigma(\mathbf{x})\) is the predicted standard deviation (or uncertainty) of the objective function at point \(\mathbf{x}\); \(\Phi(\cdot)\) is the \ac{cdf} of the standard normal distribution; and \(\phi(\cdot)\) is the \ac{pdf} of the standard Normal distribution. The next evaluation point is then selected by maximizing the \ac{ei} function:

\[
x_t = \argmax_{\mathbf{x} \in \mathcal{D}}  \alpha_{\text{EI}}(\mathbf{x})
\]

The formulation in equation \eqref{eq:ei_formula} allows computation of the \ac{ei} by combining the mean and standard deviation predicted by a \ac{gp}. Intuitively, it represents a trade-off between the standard deviation \(\sigma(\mathbf{x})\), which tends to promote exploration, and the \(z\) term which tends to promote exploitation of promising areas. This makes it a more comprehensive approach compared to other methods such as \ac{PI}, which focuses mainly on the probability of improvement, rather than its magnitude as well.

The \ac{ei} function is designed to provide a more balanced approach than the \ac{PI} acquisition function, considering both the probability and magnitude of potential improvements. The EI values are higher for locations with high predicted mean values and/or high standard deviation, since the expected improvement is a combination of the probability of improvement and the size of the improvement itself. The EI selects the point that, on average, is expected to provide the greatest improvement in the objective function relative to the best-observed value. This balanced approach allows navigating the search space in a more effective way \citep{shahriari2015taking}.

\ac{ei} is well regarded for providing a more robust balance between exploration and exploitation compared to \ac{PI}, which often results in a more efficient optimization process and faster convergence to optimal solutions \citep{frazier2018tutorial}. It has shown good empirical performance across a range of different optimization problems and has been adopted as the main acquisition function in many different Bayesian Optimization libraries. However, similar to \ac{PI}, \ac{ei} might not be suitable when specific high levels of accurate and precise exploration or exploitation are needed, since it still might make greedy choices and get trapped in local optima. Furthermore, its performance can also be affected by the accuracy of the Gaussian Process prediction, as its formulation depends on the model prediction for a Gaussian distribution.

\subsubsection{Upper Confidence Bound}
\label{section:ucb}

The \acf{ucb} acquisition function is a widely adopted strategy in Bayesian optimization, particularly when a balance between exploration and exploitation is crucial \citep{auer2002finite, srinivas2009gaussian}. \ac{ucb} operates on the principle of selecting the point with the highest upper confidence bound on its predicted value, effectively encouraging both the evaluation of points with high predicted objective values and those with high uncertainty, thus enabling a comprehensive coverage of the search space. This balance is achieved by incorporating both the predictive mean and the uncertainty (typically standard deviation) associated with that prediction into a single criterion. The \ac{ucb} strategy is appealing due to its relatively simple implementation and strong theoretical underpinnings. \ac{ucb} is a variant of the so-called "optimism in the face of uncertainty" principle, a well-established technique in the reinforcement learning field.

In the context of \ac{bo}, at the optimization iteration $t$, given a set of observations $\mathcal{D}_t$ and a probabilistic predictive model, such as a \ac{gp}, which provides a mean prediction \(\mu_t(\mathbf{x})\) and a standard deviation \(\sigma_t(\mathbf{x})\) at a new point \(\mathbf{x}\), the \ac{ucb} function is defined as:
\begin{equation}
\alpha_{\text{UCB}}(\mathbf{x}) = \mu(\mathbf{x}) + \sqrt{\beta_t} \sigma(\mathbf{x}),
\label{eq:ucb_formula}
\end{equation}
where: \(\mu_t(\mathbf{x})\) represents the predicted mean of the objective function at point \(\mathbf{x}\), obtained from the predictive model; \(\sigma(\mathbf{x})\) denotes the predicted standard deviation (or uncertainty) of the objective function at point \(\mathbf{x}\), also derived from the predictive model; and \(\beta_t\) is a hyperparameter, often referred to as the exploration-exploitation trade-off parameter, that controls the relative importance of exploration and exploitation. A higher \(\beta_t\) encourages greater exploration, prioritizing the evaluation of points with high uncertainty, which typically correspond to less sampled regions.

\ac{ucb} is relatively straightforward to implement and computationally efficient, requiring only simple arithmetic operations given the predictive mean and standard deviation. It has been shown to perform well in practice across a range of optimization problems, particularly in the early stages of optimization, because of its tendency to explore unknown regions efficiently. It is also accompanied by strong theoretical guarantees for convergence under certain conditions, which provides a formal justification for its effectiveness and is also a factor in its popularity among practitioners.

Following \citep{srinivas2009gaussian}, assuming the chosen
kernel has the smooth property for some constants $a, b, L > 0$:
\[
\mathbb{P} \left\{ \sup_{\mathbf{x} \in \mathcal{X}} \left| \frac{\partial f}{\partial x_i} \right| > L \right\} \leq a e^{-(L/b)^2}, \quad i=1 \ldots d, 
\]
then by choosing
\[
\beta_t = 2 \log \left( \frac{t^2 \pi^2}{3\delta} \right) + 2d \log \left( t^2 dbr \sqrt{\log \left( \frac{4da}{\delta} \right) } \right),
\]
where $d$ is the number of dimension, GP-\ac{ucb} algorithm is proved to achieve sublinear regret bound with probability $1 - \delta$. However, in practice, the performance of \ac{ucb} is inherently sensitive to the hyperparameter \(\beta_t\), which typically needs to be chosen by the user based on some empirical considerations and may require tuning and experimentation for each specific problem. The selection of \(\beta_t\) is not obvious and depends largely on the characteristics of the objective function. 
The simple linear combination structure used by \ac{ucb} might struggle to effectively navigate complex and highly non-linear objective function landscapes, potentially leading to suboptimal results in high-dimensional search spaces or with highly multimodal objective functions.

\subsubsection{Thompson Sampling}
\label{section:thompson_sampling}

\acf{ts}, in contrast to \ac{ucb}, is a probabilistic acquisition function that bases its decisions on samples drawn from the posterior distribution over the objective function \citep{thompson1933likelihood, russo2018tutorial}. This stochastic decision-making process provides a natural mechanism for both exploration and exploitation, in a different manner compared to the \ac{ucb} acquisition function. \ac{ts} has proven to be a powerful approach, and it is now widely adopted in reinforcement learning, Bayesian optimization, and many other fields, where it has been shown to perform very well in different settings \citep{agrawal2017thompson, chowdhury2017kernelized}.

Rather than optimizing an explicit acquisition function based on a balance of mean and uncertainty like \ac{ucb}, \ac{ts} utilizes a posterior sampling approach. 

\acf{ts} does not have an explicit analytical formula for the acquisition function itself. Instead, it relies on the following iterative process, which is grounded in sampling the posterior function:
\begin{enumerate}
    \item \textit{Posterior Sampling}: At each step $t$,  sample a function $\Hat{f}_t$ from the posterior distribution given the observed data, i.e., sample $\Hat{f}_t \sim P(f \mid \mathcal{D}_t)$. In the case of a \ac{gp} as the predictive model, this involves sampling a mean function from the posterior with the covariance also defined by the posterior. This is achieved by sampling from a multivariate Gaussian distribution. For a \ac{gp}, the posterior distribution of the objective function $f(x)$ at a new point $x$ is given by the formula in Section \ref{section:gaussian_process}. Then \ac{ts} with \ac{gp} has a general form: $\Hat{f}_t (\cdot) \sim \mathcal{GP}(\mu_t(\cdot), \nu^2 \sigma_t(\cdot))$. Here, $\nu$ us the exploration parameter and can be time-dependent. 
    
    \item \textit{Minimization}: Find the next location, $\mathbf{x}_{\text{new}}$, by minimizing the value of the sampled function $\Hat{f}(\mathbf{x})$, i.e., find the $\mathbf{x}$ such that $\mathbf{x}_{\text{new}} = \arg \min_\mathbf{x} \Hat{f}_t(\mathbf{x})$.
\end{enumerate}
This process is repeated at each step to select the next evaluation point, thus making \ac{ts} an iterative and adaptive procedure, rather than a method where one can easily calculate and evaluate an acquisition function. The power of \ac{ts} lies in this step-by-step process, which balances both the exploitation and exploration capabilities.

\ac{ts}’s inherent stochastic nature is key to its success in exploration and exploitation. Locations with higher uncertainty are more likely to be sampled due to the increased variance of the posterior distribution in those regions, which inherently introduces exploration. However, locations where the best values have been observed so far are also more likely to be sampled, leading to focused optimization. In a very natural way, \ac{ts} balances the need to explore new, unexplored regions and the need to exploit the information about previously seen promising ones. This behavior is what makes it a good candidate for Bayesian optimization, as well as many other settings.

However, the performance of \ac{ts} depends heavily on the quality of the posterior approximation and the sampling method used. For \acp{gp}, the posterior is typically a multivariate Gaussian distribution, and sampling involves drawing from this distribution at each step. In practice, the computational overhead associated with repeatedly sampling from the posterior can be higher compared to methods that rely on closed-form acquisition functions such as \ac{ucb}. For large-scale problems, this could lead to scalability issues, as sampling from the posterior involves evaluating the covariance matrix, which can become computationally expensive in high-dimensional spaces. 

The use of sampling-based methods such as \ac{ts} also introduces the possibility of overfitting, especially when the model is highly flexible or the amount of observed data is small. If the posterior distribution becomes overly confident in its predictions due to a limited number of samples, the algorithm may prematurely focus on regions that do not lead to further improvements. This can be mitigated through techniques like regularization or the use of more robust probabilistic models.

In certain cases, especially when the posterior distribution is complex, methods such as Markov Chain Monte Carlo (MCMC) are used to sample from the posterior more efficiently. While MCMC can offer more accurate posterior samples, it can also introduce additional computational complexity. The trade-off between accuracy and computational cost is an important consideration when implementing \acf{ts}.

The absence of a direct exploration-exploitation parameter, like \(\sqrt{\beta_t}\) in \ac{ucb}, allows \acf{ts} to adapt more fluidly to the problem at hand. However, its success depends on the accuracy of the posterior approximation, and its performance may degrade if the model fails to capture the true structure of the objective function. Thus, while \acf{ts} can achieve robust performance, its implementation and tuning require careful consideration of the underlying model and the sampling methods used.

\subsection{Information-Theoretic Acquisition Functions}
\label{section:information_theoretic_acquisition_functions}

Information-theoretic acquisition functions offer a different perspective on the exploration-exploitation trade-off in Bayesian optimization. Instead of directly maximizing a utility function based on predicted means and uncertainties, they aim to maximize the information gained about the objective function, specifically the location of its global optimum. By focusing on information gain, these functions can be particularly effective in scenarios where the primary goal is to minimize the uncertainty associated with the location of the optimal solution. This approach often results in more directed exploration in specific regions. The following subsections detail several widely used information-theoretic acquisition functions, discussing their mathematical formulations, intuitive motivations, and practical implications.

\subsubsection{Entropy Search}
\label{section:entropy_search}

\acf{es} proposed by \citet{hennig2012entropy} is an information-theoretic acquisition function designed to identify the next evaluation point in \ac{bo} by maximizing the expected reduction in entropy of the posterior distribution over the location of the global optimum. Unlike other methods that emphasize the function's value directly, \ac{es} concentrates on reducing the uncertainty regarding the location of the optimum, making it particularly useful in multi-modal or highly complex optimization landscapes.

Given a posterior distribution over the objective function $f$ and its corresponding location of the optimum $\mathbf{x}^*$, \ac{es} aims to maximize the expected information gain about $\mathbf{x}^*$ obtained by observing the function value at a new point $\mathbf{x}$. This process naturally links the concepts of uncertainty reduction and optimization, as reducing uncertainty about $\mathbf{x}^*$ guides the optimization process more effectively.

As presented in Section \ref{section:entropy}, the entropy of a random variable quantifies the uncertainty associated with its probability distribution. Therefore, selecting the point with maximum information about $\mathbf{x}^*$ is equivalent to selecting the point that reduces the uncertainty about its location. Mathematically, the acquisition function for \ac{es} can be expressed as:
\begin{equation}
    \alpha_{\text{ES}}(\mathbf{x}) = \mathbb{E}_{f(\mathbf{x})|\mathcal{D}} \left[H(p(\mathbf{x}^*|\mathcal{D})) - H(p(\mathbf{x}^*|\mathcal{D} \cup (\mathbf{x}, f(\mathbf{x}))))\right],
\end{equation}
where $H(\cdot)$ represents the entropy of a probability distribution, $p(\mathbf{x}^*|\mathcal{D})$ is the posterior distribution over the location of the global optimum $\mathbf{x}^*$ given the observed data $\mathcal{D}$, and $p(\mathbf{x}^*|\mathcal{D} \cup (\mathbf{x}, f(\mathbf{x})))$ is the updated posterior distribution after evaluating the objective function at $\mathbf{x}$. The expectation $\mathbb{E}_{f(\mathbf{x})|\mathcal{D}}$ is taken with respect to the predictive distribution of the function value at $\mathbf{x}$, conditioned on the observed data $\mathcal{D}$.

The term $H(p(\mathbf{x}^*|\mathcal{D}))$ quantifies the posterior entropy of the optimum location before observing the new data point $\mathbf{x}$, while $H(p(\mathbf{x}^*|\mathcal{D} \cup (\mathbf{x}, f(\mathbf{x}))))$ represents the posterior entropy after the observation. The acquisition function seeks to maximize the reduction in entropy, prioritizing the evaluation of points that contribute the most to reducing uncertainty about $\mathbf{x}^*$.

This information-theoretic perspective naturally emphasizes exploration in regions where the model is uncertain while also focusing on exploitation near promising candidates for the global optimum. As a result, \ac{es} excels in scenarios where a balance between exploration and exploitation is crucial. For instance, in multi-modal problems where local optima might mislead the optimization process, \ac{es} ensures that the search considers broader regions of the parameter space.

However, while the theoretical formulation of \ac{es} is elegant and well-suited to complex optimization problems, it is computationally intensive. The evaluation of the entropy terms and the expectation over the posterior distributions often requires solving high-dimensional integrals, which can be challenging and time-consuming. Approximations and sampling methods are typically employed to make the computations tractable, but these can introduce additional complexity and computational overhead. Consequently, while \ac{es} offers significant advantages in terms of guiding the search effectively, its application is often limited to problems with manageable computational demands or where its potential benefits justify the additional cost.

\subsubsection{Predictive Entropy Search}
\acf{pes} of \citet{hernandez2014predictive} is an advanced information-theoretic acquisition function that builds upon the principles of \ac{es}. While \ac{es} focuses on reducing the entropy of the posterior distribution over the location of the global optimum, \ac{pes} offers a computationally efficient reformulation by directly evaluating the expected reduction in entropy of the predictive distribution over the function values.

The fundamental idea behind \ac{pes} is to quantify the expected information gain about the location of the global optimum $\mathbf{x}^*$ when observing the objective function at a new point $\mathbf{x}$. However, instead of working with the posterior distribution over $\mathbf{x}^*$, \ac{pes} reformulates the acquisition function by focusing on the entropy of the predictive distribution of the function values. This reformulation simplifies the computation while preserving the core objective of reducing uncertainty about $\mathbf{x}^*$. The acquisition function for \ac{pes} is defined as:
\begin{equation}
    \alpha_{\text{PES}}(\mathbf{x}) = H(p(f(\mathbf{x})|\mathcal{D})) - \mathbb{E}_{\mathbf{x}^*|\mathcal{D}} \left[ H(p(f(\mathbf{x})|\mathcal{D}, \mathbf{x}^*)) \right],
\end{equation}
where:
\begin{itemize}
    \item $H(\cdot)$ denotes the entropy of a probability distribution.
    \item $p(f(\mathbf{x})|\mathcal{D})$ is the predictive distribution over the function value at $\mathbf{x}$ given the observed data $\mathcal{D}$.
    \item $p(f(\mathbf{x})|\mathcal{D}, \mathbf{x}^*)$ is the predictive distribution over the function value at $\mathbf{x}$, conditioned on the data $\mathcal{D}$ and the location of the optimum $\mathbf{x}^*$.
    \item $\mathbb{E}_{\mathbf{x}^*|\mathcal{D}}$ represents the expectation over the posterior distribution of the global optimum location $\mathbf{x}^*$ given the observed data $\mathcal{D}$.
\end{itemize}

The first term, $H(p(f(\mathbf{x})|\mathcal{D}))$, represents the entropy of the predictive distribution over the function value at $\mathbf{x}$ before incorporating any new information about the global optimum. The second term, $\mathbb{E}_{\mathbf{x}^*|\mathcal{D}} \left[ H(p(f(\mathbf{x})|\mathcal{D}, \mathbf{x}^*)) \right]$, represents the expected entropy of the predictive distribution after accounting for the global optimum's posterior distribution. By maximizing the difference between these terms, \ac{pes} selects the evaluation point $\mathbf{x}$ that provides the most expected information about the global optimum.

The reformulation introduced by \ac{pes} offers two significant advantages over traditional \ac{es}. First, the entropy terms in \ac{pes} are evaluated directly on the predictive distributions of function values rather than the posterior over $\mathbf{x}^*$, which can simplify computation. Second, the sampling-based approach to evaluate $\mathbb{E}_{\mathbf{x}^*|\mathcal{D}}$ enables \ac{pes} to scale better to higher-dimensional problems and more complex posterior distributions, where exact computation of the entropy terms in \ac{es} becomes intractable.

While \ac{pes} shares many strengths with \ac{es}, such as its ability to balance exploration and exploitation effectively and its suitability for multi-modal optimization problems, it also addresses some of the computational challenges inherent in \ac{es}. However, \ac{pes} still requires accurate sampling from the posterior over $\mathbf{x}^*$ and evaluations of the predictive distributions, which can introduce computational overhead and implementation complexity.

\subsubsection{Max-value Entropy Search}
\label{section:max_value_entropy_search}

\acf{mes} \citep{wang2017max} is another information-theoretic acquisition function designed to efficiently guide the search for the global optimum of a black-box function. Unlike methods such as \ac{es} and \ac{pes}, which focus on the location of the optimum or the predictive entropy of function values, \ac{mes} directly considers the uncertainty about the maximum function value itself. This focus provides a computationally efficient and intuitive way to prioritize evaluations.

The central idea behind \ac{mes} is to quantify the expected reduction in entropy of the maximum value of the objective function, $y^*$, after observing a new function evaluation. Here, $y^*$ represents the maximum value that the objective function can achieve over the entire search space. By reducing uncertainty about $y^*$, \ac{mes} implicitly narrows the search toward regions likely to contain the global optimum.

The acquisition function for \ac{mes} is defined as:
\begin{equation}
    \alpha_{\text{MES}}(\mathbf{x}) = H(p(y^*|\mathcal{D})) - \mathbb{E}_{f(\mathbf{x})|\mathcal{D}} \left[ H(p(y^*|\mathcal{D} \cup \{(\mathbf{x}, f(\mathbf{x}))\})) \right],
\end{equation}
where:
\begin{itemize}
    \item $H(\cdot)$ denotes the entropy of a probability distribution.
    \item $p(y^*|\mathcal{D})$ is the current posterior distribution over the maximum value $y^*$, given the observed data $\mathcal{D}$.
    \item $p(y^*|\mathcal{D} \cup \{(\mathbf{x}, f(\mathbf{x}))\})$ is the updated posterior distribution over $y^*$ after observing the function value $f(\mathbf{x})$ at $\mathbf{x}$.
    \item $\mathbb{E}_{f(\mathbf{x})|\mathcal{D}}$ represents the expectation over the predictive distribution of $f(\mathbf{x})$ conditioned on the observed data $\mathcal{D}$.
\end{itemize}

The first term, $H(p(y^*|\mathcal{D}))$, captures the current uncertainty about the maximum value $y^*$ across the search space, while the second term, $\mathbb{E}_{f(\mathbf{x})|\mathcal{D}} \left[ H(p(y^*|\mathcal{D} \cup \{(\mathbf{x}, f(\mathbf{x}))\})) \right]$, represents the expected uncertainty after evaluating the function at $\mathbf{x}$. By maximizing the reduction in uncertainty about $y^*$, \ac{mes} selects evaluation points that are most informative about the maximum value.

The computational efficiency of \ac{mes} stems from its use of $y^*$ as a one-dimensional summary of the optimization problem. Unlike \ac{es}, which requires sampling the posterior over the entire location of the optimum, \ac{mes} simplifies the entropy computations by working directly with the scalar value $y^*$. Sampling-based approximations, such as Monte Carlo integration, are used to estimate the entropies, making \ac{mes} practical even in higher-dimensional spaces.

\ac{mes} is particularly effective in balancing exploration and exploitation. Points with high predictive uncertainty in regions where the function could exceed the current estimate of $y^*$ are naturally prioritized, encouraging exploration of unexplored areas. Simultaneously, regions close to known high values are also favored, supporting exploitation of promising areas.

While \ac{mes} addresses many computational challenges associated with other information-theoretic methods, it still requires accurate sampling from the posterior distribution of $y^*$ and the predictive distribution of $f(\mathbf{x})$. These requirements can introduce computational overhead, especially in high-dimensional or complex posterior landscapes. However, its focused nature and simplicity often make \ac{mes} more practical and scalable compared to methods like \ac{es}.
\subsubsection{Knowledge Gradient}
\label{section:knowledge_gradient}

The \acf{kg} acquisition function in \citet{frazier2008knowledge, wu2016parallel} provides a principled way to select evaluation points by directly optimizing the improvement in the expected value of the solution to the optimization problem. Unlike entropy-based approaches such as \ac{es} or \ac{mes}, which focus on reducing uncertainty about the global optimum or maximum value, \ac{kg} measures the value of information gained from a single evaluation in terms of its impact on the expected objective value.

The key idea behind \ac{kg} is to evaluate the utility of an experiment by estimating how much it improves the decision-making process. Specifically, \ac{kg} selects the point $\mathbf{x}$ that maximizes the expected increase in the maximum posterior mean value of the objective function after observing the outcome of evaluating $f(\mathbf{x})$. Formally, the \ac{kg} acquisition function is defined as:
\begin{equation}
    \alpha_{\text{KG}}(\mathbf{x}) = \mathbb{E}_{f(\mathbf{x})|\mathcal{D}} \left[ \min_{\mathbf{x}' \in \mathcal{X}} \mu_{t+1}(\mathbf{x}') \right] - \min_{\mathbf{x}' \in \mathcal{X}} \mu_t(\mathbf{x}'),
\end{equation}
where:
\begin{itemize}
    \item $\mu_t(\mathbf{x}')$ is the posterior mean of the objective function at $\mathbf{x}'$ given the current data $\mathcal{D}$.
    \item $\mu_{t+1}(\mathbf{x}')$ is the updated posterior mean after observing the value of $f(\mathbf{x})$ at $\mathbf{x}$.
    \item $\mathbb{E}_{f(\mathbf{x})|\mathcal{D}}$ denotes the expectation over the predictive distribution of $f(\mathbf{x})$ given the observed data $\mathcal{D}$.
\end{itemize}

The first term, $\mathbb{E}_{f(\mathbf{x})|\mathcal{D}} \left[ \min_{\mathbf{x}' \in \mathcal{X}} \mu_{t+1}(\mathbf{x}') \right]$, represents the expected maximum posterior mean after evaluating $f(\mathbf{x})$. The second term, $\min_{\mathbf{x}' \in \mathcal{X}} \mu_t(\mathbf{x}')$, is the current minimum posterior mean. The \ac{kg} acquisition function seeks to maximize the difference between these two quantities, guiding evaluations to points that are expected to improve the decision about the best location.

The \ac{kg} approach has several appealing properties. By directly focusing on the impact of new information on decision-making, \ac{kg} inherently balances exploration and exploitation. Points in regions with high uncertainty are likely to lead to significant updates in the posterior mean, encouraging exploration. Meanwhile, points near current estimates of the maximum mean are prioritized for exploitation.

\ac{kg} is particularly effective in problems where evaluations are expensive, and it is crucial to optimize every experiment to maximize learning. Its ability to quantify the value of information gained from a single evaluation makes it a strong candidate for sequential optimization settings. However, implementing \ac{kg} can be computationally demanding. The expectation over the predictive distribution and the maximization over the posterior mean both require accurate approximation techniques, such as Monte Carlo integration or surrogate modeling with Gaussian processes. In high-dimensional or highly multimodal problems, the computational cost can increase significantly, requiring careful optimization strategies.

\subsection{Bandit-based Optimization}
Bandit-based optimization is a foundational framework in sequential decision-making and optimization under uncertainty. The central objective is to maximize cumulative rewards over time while balancing exploration (gathering information about suboptimal choices) and exploitation (leveraging known high-reward choices). This paradigm is often used in various applications such as recommendation systems, clinical trials, and Bayesian optimization \citep{lattimore2020bandit, bubeck2012regret}.

\subsubsection{Multi-armed Bandits}
The \ac{mab} problem is a classic formulation in the bandit optimization framework. It derives its name from a metaphorical scenario involving a gambler faced with a row of slot machines (or "one-armed bandits"), each with an unknown probability distribution of payouts. The gambler must decide which machines to play, in what order, and how many times, to maximize their total reward over a sequence of trials \citep{robbins1952some}. Formally, the \ac{mab} problem can be described as follows:
\begin{itemize}
    \item There are $K$ arms (options), indexed by $i \in \{1, 2, \ldots, K\}$.
    \item Each arm $i$ is associated with a reward distribution $P_i$, with an unknown mean reward $\mu_i = \mathbb{E}[r_i]$, where $r_i$ is the reward obtained from pulling arm $i$.
    \item At each time step $t \in \{1, 2, \ldots, T\}$, the player selects an arm $a_t \in \{1, 2, \ldots, K\}$ and observes a stochastic reward $r_{a_t} \sim P_{a_t}$.
    \item The goal is to maximize the cumulative reward over $T$ rounds:
    \begin{equation}
        R_T = \sum_{t=1}^T r_{a_t}.
    \end{equation}
\end{itemize}

The core challenge lies in the trade-off between \textit{exploration} - pulling less-sampled arms to estimate their reward distributions and \textit{exploitation} - pulling arms with known high rewards to accumulate gains. This trade-off is typically quantified using the concept of \textbf{regret}, which quantifies the cumulative loss incurred by not always selecting the optimal arm.

\paragraph{Regret Minimization}
Regret measures the difference between the cumulative reward of an optimal strategy and the actual cumulative reward obtained by a given algorithm. Formally, the \textit{cumulative regret} $R(T)$ is defined as:
\begin{equation}
    R(T) = T \mu^* - \mathbb{E}\left[\sum_{t=1}^T r_{a_t}\right],
\end{equation}
where $\mu^* = \max_{i \in \{1, \ldots, K\}} \mu_i$ is the mean reward of the optimal arm. The objective of any bandit algorithm is to minimize $R(T)$ as $T$ grows large.

\subsubsection{Algorithms for MAB}
Several algorithms have been developed to solve the MAB problem effectively. Key strategies include:

\paragraph{$\epsilon$-Greedy Algorithm}
The $\epsilon$-greedy algorithm balances exploration and exploitation by selecting the arm with the highest estimated mean reward with probability $1 - \epsilon$, and exploring a random arm with probability $\epsilon$. Formally, at each time step $t$, the algorithm selects:
\begin{equation}
    a_t =
    \begin{cases}
        \arg \max_i \hat{\mu}_i & \text{with probability } 1 - \epsilon, \\
        \text{randomly select from } \{1, \ldots, K\} & \text{with probability } \epsilon,
    \end{cases}
\end{equation}
where $\hat{\mu}_i$ is the estimated mean reward of arm $i$ based on past observations.

\paragraph{Upper Confidence Bound Algorithm}
The \ac{ucb} algorithm addresses the exploration-exploitation trade-off by assigning a confidence interval to each arm’s estimated mean reward and selecting the arm with the highest upper confidence bound. At each step $t$, the arm $a_t$ is chosen as:
\begin{equation}
    a_t = \arg \max_i \left( \hat{\mu}_i + \sqrt{\frac{2 \ln t}{n_i}} \right),
\end{equation}
where $n_i$ is the number of times arm $i$ has been pulled, and $\ln t$ encourages exploration by weighting uncertainty higher in earlier stages. The UCB algorithm is particularly effective due to its logarithmic regret bounds \citep{auer2002finite}.

\paragraph{Thompson Sampling (TS)}
\acf{ts} is a Bayesian approach to \ac{mab} problems, where posterior distributions over the mean rewards of each arm are maintained and updated based on observed rewards. At each time step, an arm is sampled from its posterior distribution and played. Formally, the algorithm selects:
\begin{equation}
    a_t = \arg \max_i \mu_i^{(t)},
\end{equation}
where $\mu_i^{(t)}$ is a sample from the posterior distribution of arm $i$. \ac{ts} has been shown to achieve asymptotically optimal regret in many settings \citep{agrawal2012analysis}.

\subsubsection{Contextual Bandits}
Contextual bandits extend the multi-armed bandit framework by introducing a context $\mathbf{c}_t \in \mathcal{C}$ at each round $t$. This context provides additional information to inform the decision-making process, allowing for more tailored arm selections. The challenge lies in effectively modeling the relationship between context and reward, as well as balancing exploration and exploitation in this extended setting. At each step, the algorithm observes a context vector $\mathbf{x}_t$ before selecting an arm. The objective is to learn a mapping from contexts to arm selection policies that maximize cumulative rewards. This framework has found widespread applications in areas such as personalized recommendations and adaptive experimentation \citep{li2010contextual}. In contrast to traditional multi-armed bandits, contextual bandits incorporate context. This integration allows for more adaptive and precise policy learning, which is crucial in dynamic and personalized environments.
 
Formally, contextual bandit problems involve a sequential decision-making process where, at each time step $t$, an agent observes a \textit{context} $\mathbf{x}_t \in \mathcal{X}$, selects an \textit{action} $a_t \in \mathcal{A}$, and receives a \textit{reward} $r_t$. Unlike standard \ac{mab} problems, contextual bandits leverage the context $\mathbf{x}_t$ to inform the action selection process.  The objective is to maximize the cumulative reward over a sequence of interactions.  Having established the problem setting of contextual bandits, we now present the general step of a standard Contextual Bandit Algorithm.

\paragraph{Steps in a Contextual Bandit Algorithm.}
A standard contextual bandit algorithm typically proceeds as follows:
\begin{enumerate}
    \item \textbf{Observe context:} A new data point arrives, described by a context $\mathbf{c}_t$ (e.g., a user with specific attributes such as device type and location).
    \item \textbf{Select action:} Based on the observed context and the exploration strategy (e.g., $\epsilon$-greedy or Thompson Sampling), the algorithm selects an action $a_t$ (e.g., a recommendation or treatment).
    \item \textbf{Receive reward:} After the action is performed, the algorithm observes the outcome $r_t$ (e.g., a user's response or purchase behavior).
    \item \textbf{Update model:} The model is updated using the new context-action-reward tuple. To reduce noise, updates are often performed in batches rather than after every single sample.
    \item \textbf{Repeat:} This process continues over multiple rounds to optimize the cumulative reward.
\end{enumerate}

In contextual bandits, the context $\mathbf{c}_t$ acts as side information that is observed before selecting an arm. This involves a trade-off between exploring different actions to learn their potential rewards and exploiting actions that are known to produce good rewards given the current context. However, unlike classical multi-armed bandits, where exploration focuses solely on the arms, contextual bandits require exploration across both arms and the context space. This dual exploration-exploitation trade-off leads to questions such as:
\begin{itemize}
    \item Should rare or less-sampled contexts be prioritized during exploration?
    \item How can the algorithm balance exploring unseen arms versus unseen contexts?
\end{itemize}
Furthermore, the success of contextual bandit algorithms heavily depends on the choice of the model used to estimate the expected reward. Hence, it introduces challenges such as modeling the reward function $r(\mathbf{c}_t, a)$, which maps context-arm pairs to rewards or managing the complexity of high-dimensional contexts while ensuring computational tractability. To address these challenges, it is crucial to select robust methods for modeling the reward function. A well-suited model not only needs to capture the underlying patterns and dependencies accurately but also maintain a balance between computational efficiency and predictive performance.

\paragraph{Modeling the Reward Function.} 
$\mathbb{E}[r_t \mid \mathbf{c}_t, a]$:
\begin{itemize}
    \item Linear models offer simplicity and computational efficiency but are limited in capturing non-linear relationships.
    \item Non-linear models, such as neural networks, provide flexibility but come at the cost of increased computational overhead and the need for larger datasets to generalize effectively.
\end{itemize}


To address these challenges, various algorithms have been proposed using both linear and non-linear models to estimate rewards. In the next subsections, we will explore some of these algorithms.

% \paragraph{Regret Bounds and Guarantees.} 
% Theoretical guarantees for contextual bandits typically rely on assumptions about the reward function. For instance:
% \begin{itemize}
%     \item When the reward function is linear, algorithms like LinUCB \citep{li2010contextual} achieve optimal regret bounds of $\mathcal{O}(\sqrt{T})$.
%     \item For non-linear functions, more general algorithms like Thompson Sampling \citep{russo2018tutorial} or neural network-based approaches \citep{riquelme2018deep} are used, but their guarantees often depend on specific problem settings.
% \end{itemize}

\subsubsection{Linear Bandits}
Linear models assume that the expected reward for a given context and action can be represented as a linear combination of some features. Specifically, we assume there exists an unknown parameter vector $\boldsymbol{\theta}$ such that the expected reward can be approximated by the inner product of this vector with a context-action feature vector:
\begin{equation}
    \mathbb{E}[r_t | \mathbf{c}_t, a_t] = \boldsymbol{\theta}^T \phi(\mathbf{c}_t, a_t)
\end{equation}
where $\phi(\mathbf{c}_t, a_t)$ is the feature mapping of context $\mathbf{c}_t$ and action $a_t$ into a feature vector.

\paragraph{Linear Upper Confidence Bound (LinUCB) Algorithm}
LinUCB operates by maintaining an estimate of the parameter vector $\boldsymbol{\theta}$, and using confidence bounds to balance exploration and exploitation. It calculates an upper confidence bound on the expected reward using a known hyper-parameter $\alpha$ to balance exploration/exploitation trade-off. At each time step, the action that maximizes the upper confidence bound is selected. The algorithm works as follows:
    \begin{enumerate}
        \item Initialize $\mathbf{A}_0 = \mathbf{I}$ (identity matrix), $\mathbf{b}_0 = \mathbf{0}$, $\hat{\boldsymbol{\theta}}_0 = \mathbf{0}$.
        \item At each time step $t$:
          \begin{enumerate}
              \item Observe context $\mathbf{x}_t$.
              \item For each action $a \in \mathcal{A}$, compute:
              \begin{align*}
                \hat{r}_t(a) &= \hat{\boldsymbol{\theta}}_{t-1}^T \phi(\mathbf{x}_t, a) + \alpha \sqrt{ \phi(\mathbf{x}_t, a)^T \mathbf{A}_{t-1}^{-1} \phi(\mathbf{x}_t, a)}
              \end{align*}
              \item Select action $a_t = \text{argmax}_{a \in \mathcal{A}} \hat{r}_t(a)$.
              \item Observe reward $r_t$.
              \item Update:
              \begin{align*}
                  \mathbf{A}_t &= \mathbf{A}_{t-1} + \phi(\mathbf{x}_t, a_t) \phi(\mathbf{x}_t, a_t)^T \\
                  \mathbf{b}_t &= \mathbf{b}_{t-1} + r_t \phi(\mathbf{x}_t, a_t) \\
                  \hat{\boldsymbol{\theta}}_t &= \mathbf{A}_t^{-1} \mathbf{b}_t
              \end{align*}
          \end{enumerate}
    \end{enumerate}
\paragraph{Thompson Sampling for Linear Models}: \acf{ts} for linear bandits also attempts to learn the weight vector $\boldsymbol{\theta}$ by maintaining a posterior distribution over these weights. At each step, the agent samples a weight vector from the posterior, and uses this weight vector to select the action that maximizes the reward. The algorithm works as follows:
    \begin{enumerate}
    \item Initialize the posterior using a prior distribution for $\boldsymbol{\theta}$ and a covariance matrix. Usually a gaussian distribution is used for the prior.
    \item At each time step $t$:
      \begin{enumerate}
        \item Observe context $\mathbf{x}_t$.
        \item Sample parameter $\hat{\boldsymbol{\theta}}_t$ from posterior distribution.
        \item Select action $a_t = \text{argmax}_{a \in \mathcal{A}} \hat{\boldsymbol{\theta}}_t^T \phi(\mathbf{x}_t, a)$.
        \item Observe reward $r_t$.
        \item Update the posterior distribution based on observation.
      \end{enumerate}
    \end{enumerate}

Linear models offer several advantages, making them a popular choice in contextual bandit algorithms. They are computationally efficient, requiring fewer resources and enabling faster computations, which is particularly valuable in large-scale applications. Their simplicity makes them easier to understand and implement, reducing the complexity of algorithm design. Additionally, linear models often come with well-established theoretical guarantees, providing performance bounds that enhance their reliability. However, these models also have notable limitations. They struggle to capture complex non-linear relationships, which can result in poor performance when the underlying connection between context, action, and reward deviates from linearity. Furthermore, their effectiveness heavily depends on the quality of feature mappings, as the performance hinges on well-designed features in $\phi(\mathbf{x}_t, a_t)$. To mitigate these limitations, proper feature engineering and careful tuning of hyperparameters are critical to achieving good performance with linear bandit algorithms.
To address the limitations of linear models in capturing complex relationships, non-linear models offer a more flexible and expressive alternative. 


\subsubsection{Non-Linear Bandits}
Non-linear models relax the linearity assumption and allow the expected reward for a given context and action to be represented by more complex, non-linear functions. These models aim to capture intricate relationships between the context, action, and reward, which are often observed in real-world scenarios. Formally, we assume the expected reward can be expressed as:
\begin{equation}
    \mathbb{E}[r_t | \mathbf{c}_t, a_t] = f(\mathbf{x}_t, \boldsymbol{theta_t}),
\end{equation}
where $f$ is a non-linear function, such as a neural network, and $\mathbf{x}_t  = \phi(\mathbf{c}_t, a_t)$ is the feature mapping of the context $\mathbf{c}_t$ and action $a_t$.

\paragraph{Neural Upper Confidence Bound (NeuralUCB) Algorithm}
NeuralUCB extends the idea of LinUCB by using a neural network to model the reward function. Instead of maintaining a linear estimate of $\boldsymbol{\theta}$, the algorithm trains a neural network to approximate the reward function, while maintaining an exploration bonus based on the model’s uncertainty. The steps are as follows:
\begin{enumerate}
    \item Initialize the neural network $f_\theta$ with random weights and set the exploration parameter $\alpha$.
    \item At each time step $t$:
    \begin{enumerate}
        \item Observe context $\mathbf{x}_t$.
        \item For each action $a \in \mathcal{A}$, compute:
        \begin{align*}
            \hat{r}_t(a) &= f_\theta(\phi(\mathbf{x}_t, a)) + \alpha \sqrt{u_t(\phi(\mathbf{x}_t, a))},
        \end{align*}
        where $u_t$ is an uncertainty estimate (e.g., derived from a Bayesian approximation or ensemble models).
        \item Select action $a_t = \text{argmax}_{a \in \mathcal{A}} \hat{r}_t(a)$.
        \item Observe reward $r_t$ and update the neural network parameters $\theta$ to minimize the prediction error.
    \end{enumerate}
\end{enumerate}

\paragraph{Thompson Sampling for Non-Linear Models}
Thompson Sampling can also be extended to non-linear models by maintaining a distribution over the parameters of the non-linear model (e.g., neural network weights). At each time step, the algorithm samples a model from the posterior distribution, selects the action that maximizes the reward based on the sampled model, and updates the posterior using the observed data. The steps are as follows:
\begin{enumerate}
    \item Initialize a prior distribution over the parameters of the non-linear model (e.g., a Bayesian neural network or ensemble).
    \item At each time step $t$:
    \begin{enumerate}
        \item Observe context $\mathbf{x}_t$.
        \item Sample model parameters $\hat{\theta}_t$ from the posterior distribution.
        \item Select action $a_t = \text{argmax}_{a \in \mathcal{A}} f_{\hat{\theta}_t}(\phi(\mathbf{x}_t, a))$.
        \item Observe reward $r_t$ and update the posterior distribution.
    \end{enumerate}
\end{enumerate}

Non-linear models address the limitations of linear models by providing the flexibility to capture complex and non-linear relationships in the data. This makes them particularly effective in scenarios with intricate dependencies between context, action, and reward. However, this flexibility comes with trade-offs. Non-linear models often require significantly more computational resources and larger datasets to achieve reliable generalization. Moreover, they are more prone to overfitting and require careful regularization and hyperparameter tuning. Despite these challenges, advances in neural network architectures and training techniques have made non-linear models increasingly viable for bandit problems, enabling their application to more complex and high-dimensional contexts.


\paragraph{Introduction to Non-Linear Models}
Non-linear models extend the capabilities of contextual bandits to handle situations where the reward function cannot be adequately approximated using linear models. These models allow for more complex relationships between contexts, actions, and rewards.

\paragraph{Key Algorithms}
\begin{itemize}
    \item \textbf{Kernelized Bandits (KernelUCB, KernelTS)}:
    Kernel methods are an alternative to feature-mapping based approaches. They map the data into a higher-dimensional space using a kernel function $k(x_i,x_j)$. Kernelized bandits operate by maintaining estimates of the reward function in the kernel space. KernelUCB and KernelTS are extensions of LinUCB and Thompson Sampling for linear models, which utilize the kernel trick to implicitly perform the computation of the features in the high-dimensional feature space.

    \item \textbf{Neural Network Based Bandits}: Neural networks can be used to model non-linear reward functions. A neural network can be trained to approximate the reward given a specific context and action. These models can be incorporated in bandit algorithms using strategies like:
        \begin{itemize}
            \item Using $\epsilon$-greedy strategy using neural network to predict best action.
            \item Incorporating uncertainty in the neural network outputs, and then using UCB or Thompson sampling strategies to balance exploration/exploitation.
        \end{itemize}
\end{itemize}

\paragraph{Advantages of Non-Linear Models}
\begin{itemize}
    \item \textbf{Flexibility}: They can capture more complex non-linear relationships, potentially leading to improved performance when a linear assumption is inappropriate.
    \item \textbf{Improved Performance}: Non-linear models often achieve superior results in scenarios where complex relationships exist between the context, action, and the reward.
\end{itemize}

\paragraph{Limitations of Non-Linear Models}
\begin{itemize}
    \item \textbf{Computational Costs}: Non-linear models are generally more computationally intensive and can be slower to train and apply, specially compared to linear methods.
    \item \textbf{Need for more data}: Require more data for training to avoid overfitting.
    \item \textbf{Hyperparameter Tuning}: Require careful tuning of hyperparameters to achieve optimal performance.
\end{itemize}

\subsection{Performance Metrics in Black-box Optimization}
\label{background:performance_metrics}
Evaluating the performance of a \acf{bo} algorithm involves quantifying its efficiency in identifying the optimal solution of the objective function \( f \). Two commonly used performance metrics in the literature are \emph{simple regret} and \emph{cumulative regret}, which capture different aspects of the optimization process.

\subsubsection{Simple Regret}
The \emph{simple regret} measures the difference between the best function value obtained after \( t \) evaluations and the true global maximum. Formally, it is defined as:
\[
r_t = f(\mathbf{x}^*) - f(\mathbf{x}_t^*),
\]
where:
\begin{itemize}
    \item \( f(\mathbf{x}^*) = \max_{\mathbf{x} \in \mathcal{D}} f(\mathbf{x}) \) is the global maximum of the objective function \( f \),
    \item \( \mathbf{x}_t^* = \arg \max_{i=1,\dots,t} f(\mathbf{x}_i) \) is the best input queried among the \( t \) evaluations.
\end{itemize}

Simple regret evaluates the quality of the best solution found so far without considering the total cost incurred during the optimization process. This metric is particularly relevant for applications where the goal is to identify a high-quality solution at the end of the optimization rather than during the intermediate steps.

\subsubsection{Cumulative Regret}

The \emph{cumulative regret} quantifies the total loss incurred due to querying suboptimal points during the optimization process with optimization budget $T$. It is defined as:
\[
R_T = \sum_{t=1}^T \left( f(\mathbf{x}^*) - f(\mathbf{x}_t) \right),
\]
where \( \mathbf{x}_t \) denotes the point queried at the \( t \)-th step. 

Cumulative regret provides a measure of how efficiently the algorithm balances exploration and exploitation over \( t \) iterations. A lower cumulative regret indicates that the algorithm has made better use of its queries to approach the global optimum efficiently.

\subsubsection{Comparison and Relevance}

Simple regret and cumulative regret capture different trade-offs in BO. Simple regret focuses solely on the quality of the final solution and is particularly suited for offline optimization problems, where the cost of intermediate evaluations is less critical. In contrast, cumulative regret is more relevant in settings where every evaluation incurs a cost, such as online optimization, and emphasizes the efficiency of the entire optimization process.

Both metrics are essential for evaluating the performance of BO algorithms, offering complementary insights into their behavior and effectiveness across various applications.

\section{Black-box Optimization with Unknown Black-box Constraints}
\label{section:bo_unknown_constraints}
As real-world problems often involve constraints that are also black-box in nature, Constrained Black-box Optimization (CBO) has become a vital extension of BO. CBO methods adjust the acquisition function to account for these constraints, seeking feasible solutions that satisfy the conditions while optimizing the objective function. A prominent method in CBO is the Expected Improvement with Constraints (cEI), first introduced by \citep{schonlau1998global} and later extended by \citep{gardner2014bayesian} and \citep{gelbart2014bayesian}. cEI integrates feasibility into the acquisition function, directing the optimization process toward regions where feasible solutions are likely. \citet{letham2019constrained} further improved cEI by using a quasi-Monte Carlo approximation to better manage observation noise, enhancing its effectiveness in noisy environments.

EI-based methods for constrained optimization face several challenges. When no feasible point exists, the EI cannot be computed, leading to modifications that focus solely on finding feasible regions, ignoring the objective function. Additionally, numerical challenges further limit some methods like EVR and IECI to small-dimensional problems. To address this, alternative methods have been proposed. For example, Predictive Entropy Search with Constraints (PESC,  \citealp{hernandez2015predictive}) offers a heuristic approach that selects feasible candidates directly from the search space, reducing uncertainty more effectively. However, the computational challenges associated with quadrature calculations during sampling have limited its practical applicability. Recently, \citet{takeno2022sequential} proposed a Min-Value Entropy Search method that simplifies the sampling process, making it more tractable.

Numerical optimization has also been taken into consideration as an effective tool for solving the unknown constraint problem. The idea is to reformulate constraints into simpler unconstrained problems solved through alternating iterations. The Augmented Lagrangian method is mostly used in this category. For example, \citet{gramacy2016modeling} with Augmented Lagrangian Bayesian Optimization (ALBO) and its improvement Slack-AL \citep{picheny2016bayesian} use Augmented Lagrangian Function (ALF) to formulate unconstrained surrogate problems and then solve them using EI as an acquisition function. Recently, ADMMBO \citep{ariafar2019admmbo} first applied the ADMM technique to transform the constrained problem into an equivalent unconstrained optimization, then solved an augmented Lagrangian relaxation. However, this method requires the introduction of additional variables, leading to increased computational costs.

Recent research has explored penalty functions and primal-dual methods to handle constraint violations during optimization. For example, \citet{lu2022no} introduced a penalty-function approach that adds a penalty term for constraint violations to the objective function, transforming the constrained problem into an unconstrained one. Similarly, \citep{zhou2022kernelized} proposed a primal-dual approach that balances the trade-off between optimizing the objective and minimizing constraint violations. While these methods are promising, their effectiveness is sensitive to the choice of parameters set, often requiring considerable effort in parameter tuning during implementation.

Alongside empirical advancements, recent theoretical works have started to address the absence of formal guarantees in Constrained Black-box Optimization (CBO). For example, \citet{lu2022no} introduced a penalty-based regret bound that combines the regret from the objective function with penalties for constraint violations. \citet{xu2023constrained} expanded this analysis by separately evaluating cumulative regret and constraint violations. In contrast, \citet{nguyen2023optimistic} provided a theoretical performance guarantee for CBO under unknown constraints in a \textit{decoupled} setting, where cumulative regret is calculated as the sum of both objective function regret and constraint violations.
\section{Black-box Optimization with Physical Information}
\label{section:bo_physics}

In the realm of objective functions encountered in scientific and engineering domains, many are governed by \acfp{pde}. These equations encapsulate the fundamental laws of physics that describe how systems evolve over time and space. For example, the heat equations describe the distribution of heat in a given space over time, the Navier-Stokes equations describe how the velocity, pressure, and density of a fluid change over time and space. They take into account factors such as the viscosity (resistance to flow) of the fluid and external forces acting upon it. Furthermore, physical laws being implied in \ac{pde} can also be found in structural analysis or electromagnetics, to name a few. Notably, recent efforts have been made to integrate PDE knowledge into objective function models. \citet{raissi2017machine}introduced a new method using Gaussian Processes Regression (GPR) with a unique four-block covariance kernel, enabling the utilization of observations from both the objective function and the \acp{pde}. Another approach, as described in \cite{jidling2017linearly}, proposes a specialized covariance kernel to constrain \acfp{gp} using differential equations. Unlike the method in \cite{raissi2017machine}, this approach enforces the constraint globally instead of relying on specific data points. This not only provides a stronger constraint but also eliminates the computational burden associated with the four-block covariance matrix. For more details, see \cite{swiler2020survey}. Recently, \cite{chen2021solving} proposed a numerical algorithm to approximate the solution of a given non-linear \ac{pde} as a Maximum a Posteriori (MAP) estimator of a \ac{gp} conditioned on a finite set of data points from the \ac{pde}. Remarkably, their approach offers guaranteed convergence for a broad and inclusive class of \acp{pde}. Despite the promising potential of \acp{gp} in solving \acp{pde}, \acp{gp} have a limitation as their computational scalability is a critical problem. The kernel matrix inversion when updating the posterior of a \ac{gp} exhibits cubic complexity in the number of data points.      


Recently, the Physics-Informed Neural Network (PINN) was introduced \cite{raissi2019physics,yang2021b}, which offers a viable approach for tackling general PDEs. Unlike GP, PINN leverages the expressive power of neural networks to approximate complex, nonlinear relationships within the data. This inherent flexibility enables PINN to handle a broader range of PDEs including nonlinear PDEs, making them well-suited for diverse scientific and engineering applications. Further, there is research providing a deeper insight into the theoretical aspect of incorporating the PDEs using PINN \cite{schiassi2021extreme,wangL2}, and analyzing the connection between GP and PINN models to learn the underlying functions that satisfy PDE equations \cite{wang2022and}. 

As the PDEs hold promise as a valuable source of information, they could greatly improve the modeling of the black-box function and thus help towards sample-efficient optimization, reducing the number of function evaluations. It is worth noting that there has been relatively little exploration of how PDEs can be leveraged in the context of black-box optimization settings, where the objective function is treated as an unknown and potentially noisy function. 
In this paper, we consider a global optimization problem setting where the objective function $f \colon \mathcal{D} \rightarrow \mathbb{R}$ is associated with a PDE:   
\begin{equation*}
        \underset{\mathbf{x} \in \mathcal{D}}{\min} f(\mathbf{x}) \text{ s.t. }  \mathcal{N}[f](\mathbf{x}) = g(\mathbf{x}),
\end{equation*} 
where $\mathcal{D} \subset \mathbb{R}^d$ is a $d$-dimensional bounded domain and $\mathcal{N}[f]$ denotes a differential operator of the function $f$ with respect to the input $\mathbf{x}$. The function $f$ is an expensive, black-box function, and its evaluations are obtainable only through noisy measurements in the form of $y = f(\mathbf{x}) + \epsilon$, where $\epsilon$ represents sub-Gaussian noise, as elaborated later in Section \ref{section:theoretical_analysis}. Additionally, the function $g(\mathbf{x})$ is a cheap-to-evaluate function, which may also involve noise, with respect to the PDE-constraint. For example, consider the damped harmonic oscillator from classical physics as a real-world example where $f(x)$ is the displacement of the oscillator as a black-box function of time $x$, and measuring the displacement of a damped harmonic oscillator accurately can be expensive due to several factors, e.g., instrumentation cost, environmental factors, and calibration requirements. This displacement is governed by the PDE: $m \frac{d^2f}{dx^2} + c \frac{df}{dx} + kf = 0$, where $\mathcal{N}[f](x) = m \frac{d^2f}{dx^2} + c \frac{df}{dx} + kf$, $g(x)=0$, and $m, c, k$ are mass, damping and spring values, respectively. Furthermore, it is assumed that the boundary conditions of the PDEs are either unknown or inaccessible. These assumptions widely hold in many problem settings. As an example, \cite{cai2020heat} examines a two-dimensional heat transfer problem with forced heat convection around a circular cylinder. The heat measurement entails high costs due to the material, size, and shape of the system. The problem has known incompressible Navier-Stokes and heat transfer equations. However, the thermal boundary conditions are difficult to ascertain precisely because of the complex and large instruments. The unavailability of these boundary conditions prevents the straightforward solution of the underlying function $f$ using traditional numerical techniques. 

\section{Further Research in Black-box Optimisation}
\label{section:bo_further_research}


